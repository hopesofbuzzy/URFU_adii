{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "53AgVEcWMmES"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hopesofbuzzy/URFU_adii/blob/main/%D0%9E%D0%9F%D0%94/clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Средний вариант с fuzzy-search"
      ],
      "metadata": {
        "id": "53AgVEcWMmES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "vKm20zAXPS3r",
        "outputId": "dde89fa0-89c0-46f9-e890-0f6d7bbdc971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (3.14.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_and_cluster_colab_L12.py\n",
        "\n",
        "# Установка зависимостей (нужно запустить один раз)\n",
        "# pip install sentence-transformers scikit-learn rapidfuzz numpy\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import process\n",
        "import re\n",
        "\n",
        "# Пути к файлам (в Colab файлы лежат в корне по умолчанию)\n",
        "MESSAGES_FILE = \"messages.json\"\n",
        "CLUSTERS_FILE = \"clusters_L12.json\"\n",
        "\n",
        "# Название модели (загрузится автоматически)\n",
        "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Параметры\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.65\n",
        "EPS_DBSCAN = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "MIN_SAMPLES = 2\n",
        "GEOCODE_SIMILARITY_THRESHOLD = 0.1\n",
        "\n",
        "# Словарь топонимов Екатеринбурга\n",
        "# Словарь топонимов Екатеринбурга\n",
        "# Словарь топонимов Екатеринбурга\n",
        "TOPONYMS = [\n",
        "    \"Верх-Исетский\",\n",
        "    \"Железнодорожный\",\n",
        "    \"Кировский\",\n",
        "    \"Ленинский\",\n",
        "    \"Октябрьский\",\n",
        "    \"Орджоникидзевский\",\n",
        "    \"Чкаловский\",\n",
        "    \"Академический\",\n",
        "    \"Вторчермет\",\n",
        "    \"Втузгородок\",\n",
        "    \"Горный Щит\",\n",
        "    \"Елизаветинское\",\n",
        "    \"ЖБИ\",\n",
        "    \"Завокзальный\",\n",
        "    \"Изумрудный\",\n",
        "    \"Кольцово\",\n",
        "    \"Комсомольский\",\n",
        "    \"Короленковский\",\n",
        "    \"Малый Исток\",\n",
        "    \"Метеогорка\",\n",
        "    \"Нижнесвердловский\",\n",
        "    \"Новая Сортровка\",\n",
        "    \"Новобелореченский\",\n",
        "    \"Павелшина\",\n",
        "    \"Парковый\",\n",
        "    \"Пионерский\",\n",
        "    \"Психбольница\",\n",
        "    \"Рудничный\",\n",
        "    \"Семь Ключей\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Синие Камни\",\n",
        "    \"Сортировка\",\n",
        "    \"Старая Сортровка\",\n",
        "    \"Старый Октябрь\",\n",
        "    \"Татищева\",\n",
        "    \"Уктус\",\n",
        "    \"УНЦ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Центр\",\n",
        "    \"Черкасская\",\n",
        "    \"Шабровский\",\n",
        "    \"Шарташ\",\n",
        "    \"Широкая Речка\",\n",
        "    \"Эльмаш\",\n",
        "    \"Юго-Западный\",\n",
        "    \"8 Марта\",\n",
        "    \"Амундсена\",\n",
        "    \"Авиационная\",\n",
        "    \"Академика Павлова\",\n",
        "    \"Бабушкина\",\n",
        "    \"Бахчиванджи\",\n",
        "    \"Белинского\",\n",
        "    \"Бориса Ельцина\",\n",
        "    \"Братиславская\",\n",
        "    \"Бродова\",\n",
        "    \"Бульвар Академика Семихатова\",\n",
        "    \"Бутюлина\",\n",
        "    \"Валдайская\",\n",
        "    \"Верхняя Пышма\",\n",
        "    \"Вилонова\",\n",
        "    \"Водная\",\n",
        "    \"Волгоградская\",\n",
        "    \"Генеральская\",\n",
        "    \"Героев России\",\n",
        "    \"Глинки\",\n",
        "    \"Гоголя\",\n",
        "    \"Горького\",\n",
        "    \"Декабристов\",\n",
        "    \"Донбасская\",\n",
        "    \"Дружининская\",\n",
        "    \"Ереванская\",\n",
        "    \"Жуковского\",\n",
        "    \"Заводская\",\n",
        "    \"Зои Космодемьянской\",\n",
        "    \"Кирова\",\n",
        "    \"Колмогорова\",\n",
        "    \"Комсомольская\",\n",
        "    \"Короленко\",\n",
        "    \"Космонавтов\",\n",
        "    \"Крауля\",\n",
        "    \"Куйбышева\",\n",
        "    \"Лермонтова\",\n",
        "    \"Луначарского\",\n",
        "    \"Малышева\",\n",
        "    \"Мамина-Сибиряка\",\n",
        "    \"Маршала Жукова\",\n",
        "    \"Машинная\",\n",
        "    \"Мельникайте\",\n",
        "    \"Металлургов\",\n",
        "    \"Мира\",\n",
        "    \"Октябрьская\",\n",
        "    \"Павла Шпагина\",\n",
        "    \"Победы\",\n",
        "    \"Пролетарская\",\n",
        "    \"Проспект Космонавтов\",\n",
        "    \"Радищева\",\n",
        "    \"Репина\",\n",
        "    \"Розы Люксембург\",\n",
        "    \"Сакко и Ванцетти\",\n",
        "    \"Свердлова\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Софьи Ковалевской\",\n",
        "    \"Студенческая\",\n",
        "    \"Татищева\",\n",
        "    \"Техническая\",\n",
        "    \"Тургенева\",\n",
        "    \"Туполева\",\n",
        "    \"Учительская\",\n",
        "    \"Фрунзе\",\n",
        "    \"Чапаева\",\n",
        "    \"Чкалова\",\n",
        "    \"Шарташская\",\n",
        "    \"Шефская\",\n",
        "    \"Шиловская\",\n",
        "    \"Щорса\",\n",
        "    \"Энгельса\",\n",
        "    \"Площадь 1905 года\",\n",
        "    \"Площадь Труда\",\n",
        "    \"ЦУМ\",\n",
        "    \"Геологическая\",\n",
        "    \"ВИЗ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Сакко\",\n",
        "    \"Геолка\",\n",
        "    \"1905\",\n",
        "    \"Уралмашь\",\n",
        "    \"Выксунский завод\",\n",
        "    \"Монтажников\",\n",
        "    \"Расточная\",\n",
        "    \"Екатеринбург\",\n",
        "    \"Расточная\",\n",
        "    \"Монтажников\",\n",
        "    \"УрФУ\",\n",
        "    \"НВК\",\n",
        "    \"Новокольцовский\",\n",
        "    \"НВК\"\n",
        "]\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s\\dа-яё]', ' ', text.lower())\n",
        "\n",
        "def extract_geocodes(text: str) -> list[str]:\n",
        "    text = normalize_text(text)\n",
        "    found = set()\n",
        "    for word in text.split():\n",
        "        match, score, _ = process.extractOne(word, TOPONYMS)\n",
        "        if score >= 75:\n",
        "            found.add(match)\n",
        "    return list(found)\n",
        "\n",
        "def geocode_similarity(g1: list[str], g2: list[str], threshold=GEOCODE_SIMILARITY_THRESHOLD) -> bool:\n",
        "    if not g1 or not g2:\n",
        "        return False\n",
        "    set1 = set(g1)\n",
        "    set2 = set(g2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    if union == 0:\n",
        "        return False\n",
        "    return (intersection / union) >= threshold\n",
        "\n",
        "def cluster_messages(messages, model):\n",
        "    # Добавляем geocode к каждому сообщению\n",
        "    for msg in messages:\n",
        "        msg[\"geocode\"] = extract_geocodes(msg[\"text\"])\n",
        "\n",
        "    # Группируем сообщения по схожести geocode\n",
        "    clusters_by_geo = []\n",
        "    for msg in messages:\n",
        "        assigned = False\n",
        "        for geo_cluster in clusters_by_geo:\n",
        "            if geocode_similarity(msg[\"geocode\"], geo_cluster[0][\"geocode\"]):\n",
        "                geo_cluster.append(msg)\n",
        "                assigned = True\n",
        "                break\n",
        "        if not assigned:\n",
        "            clusters_by_geo.append([msg])\n",
        "\n",
        "    clusters = []\n",
        "    cluster_id_counter = 1\n",
        "\n",
        "    for geo_group in clusters_by_geo:\n",
        "        if len(geo_group) < 2:\n",
        "            continue\n",
        "\n",
        "        texts = [m[\"text\"] for m in geo_group]\n",
        "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "        clustering = DBSCAN(eps=EPS_DBSCAN, min_samples=MIN_SAMPLES, metric='cosine').fit(embeddings)\n",
        "\n",
        "        cluster_map = {}\n",
        "        for i, label in enumerate(clustering.labels_):\n",
        "            if label == -1:\n",
        "                continue\n",
        "            if label not in cluster_map:\n",
        "                cluster_map[label] = []\n",
        "            cluster_map[label].append(geo_group[i])\n",
        "\n",
        "        for cluster_label, cluster_msgs in cluster_map.items():\n",
        "            if len(cluster_msgs) < 2:\n",
        "                continue\n",
        "\n",
        "            primary_geo = cluster_msgs[0][\"geocode\"]\n",
        "            core_emb = np.mean(model.encode([m[\"text\"] for m in cluster_msgs], convert_to_numpy=True), axis=0)\n",
        "            cluster = {\n",
        "                \"cluster_id\": f\"cl_{cluster_id_counter:03d}\",\n",
        "                \"geocode\": primary_geo,\n",
        "                \"message_count\": len(cluster_msgs),\n",
        "                \"first_seen\": min(m[\"date\"] for m in cluster_msgs),\n",
        "                \"last_seen\": max(m[\"date\"] for m in cluster_msgs),\n",
        "                \"core_embedding\": core_emb.tolist(),\n",
        "                \"examples\": cluster_msgs\n",
        "            }\n",
        "            clusters.append(cluster)\n",
        "            cluster_id_counter += 1\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def main():\n",
        "    print(\"Загрузка модели L12...\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')  # Colab может использовать GPU, если доступно\n",
        "\n",
        "    print(\"Загрузка сообщений...\")\n",
        "    with open(MESSAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        messages = json.load(f)\n",
        "\n",
        "    print(f\"Обнаружено {len(messages)} сообщений.\")\n",
        "\n",
        "    print(\"Извлечение геокодов и кластеризация...\")\n",
        "    clusters = cluster_messages(messages, model)\n",
        "\n",
        "    print(f\"Создано {len(clusters)} кластеров.\")\n",
        "\n",
        "    print(\"Сохранение кластеров...\")\n",
        "    with open(CLUSTERS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"clusters\": clusters}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "from natasha import Doc, MorphVocab, NewsEmbedding, NewsNERTagger, NewsMorphTagger\n",
        "from razdel import sentenize, tokenize\n",
        "\n",
        "def extract_geo_with_natasha(text: str) -> list[str]:\n",
        "    doc = Doc(text)\n",
        "    embedding = NewsEmbedding()\n",
        "    morph_tagger = NewsMorphTagger(embedding)\n",
        "    ner_tagger = NewsNERTagger(embedding)\n",
        "\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    doc.tag_ner(ner_tagger)\n",
        "\n",
        "    geos = []\n",
        "    for span in doc.spans:\n",
        "        if span.type == 'LOC':  # или 'GPE' — Geopolitical Entity\n",
        "            geos.append(span.text)\n",
        "    return geos\n",
        "\n",
        "def extract_geocodes_enhanced(text: str) -> list[str]:\n",
        "    found = set()\n",
        "\n",
        "    # 1. Fuzzy-поиск по словарю\n",
        "    text_norm = normalize_text(text)\n",
        "    for word in text_norm.split():\n",
        "        match, score, _ = process.extractOne(word, TOPONYMS)\n",
        "        if score >= 85:\n",
        "            found.add(match)\n",
        "\n",
        "    # 2. Извлечение через NER (например, natasha)\n",
        "    ner_geos = extract_geo_with_natasha(text)\n",
        "    for geo in ner_geos:\n",
        "        # Сопоставляем с каноническим словарём\n",
        "        match, score, _ = process.extractOne(geo, TOPONYMS)\n",
        "        if score >= 80:\n",
        "            found.add(match)\n",
        "\n",
        "    # 3. Регулярки для паттернов\n",
        "    patterns = [\n",
        "        r'на\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"на Ленина\"\n",
        "        r'улица\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"улица Ленина\"\n",
        "        r'ул\\.\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"ул. Ленина\"\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        for m in matches:\n",
        "            match, score, _ = process.extractOne(m, TOPONYMS)\n",
        "            if score >= 85:\n",
        "                found.add(match)\n",
        "\n",
        "    return list(found)\n",
        "\n",
        "    print(f\"✅ Обработка завершена. Результат в {CLUSTERS_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  extract_geocodes_enhanced(\"В Кольцово объяснили, почему чемоданы пассажиров валялись на снегу.Как казалось, одна из телег наехала на снежный накат и выронила несколько чемоданов. Водитель автопоезда заметил падение багажа и вернул груз обратно.Отметим, что на взлетно-посадочную полосу багаж, ко\")\n",
        "    # main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "QXfnwF36cvrA",
        "outputId": "8444da05-188b-4978-989f-62c52beda9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2766330278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0mextract_geocodes_enhanced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"В Кольцово объяснили, почему чемоданы пассажиров валялись на снегу.Как казалось, одна из телег наехала на снежный накат и выронила несколько чемоданов. Водитель автопоезда заметил падение багажа и вернул груз обратно.Отметим, что на взлетно-посадочную полосу багаж, ко\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;31m# main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2766330278.py\u001b[0m in \u001b[0;36mextract_geocodes_enhanced\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# 2. Извлечение через NER (например, natasha)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mner_geos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_geo_with_natasha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgeo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mner_geos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Сопоставляем с каноническим словарём\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2766330278.py\u001b[0m in \u001b[0;36mextract_geo_with_natasha\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mner_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewsNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/natasha/doc.py\u001b[0m in \u001b[0;36mtag_morph\u001b[0;34m(self, tagger)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mtag_morph_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_syntax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/natasha/doc.py\u001b[0m in \u001b[0;36mtag_morph_doc\u001b[0;34m(doc, tagger)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtag_morph_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mmarkups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4hg6oRytNis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Хороший вариант с spacy (без LLM)\n"
      ],
      "metadata": {
        "id": "0qncwRwMtMSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers scikit-learn rapidfuzz numpy spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObBLZjzkwqMe",
        "outputId": "ce106b52-e387-4019-db5c-69c2f11a5c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.14.3\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_cluster_pair.py\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Название модели\n",
        "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Параметры\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.5  # Порог косинусной близости\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def test_pair(text1: str, text2: str, model, threshold: float):\n",
        "    emb1 = model.encode([text1], convert_to_numpy=True)[0]\n",
        "    emb2 = model.encode([text2], convert_to_numpy=True)[0]\n",
        "\n",
        "    sim = cosine_similarity(emb1, emb2)\n",
        "    print(f\"Текст 1: {text1}\")\n",
        "    print(f\"Текст 2: {text2}\")\n",
        "    print(f\"Схожесть: {sim:.4f}\")\n",
        "    print(f\"Порог: {threshold}\")\n",
        "    print(f\"Результат: {'✅ Объединены' if sim >= threshold else '❌ Не объединены'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "def main():\n",
        "    print(\"Загрузка модели...\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')  # или 'cuda', если GPU доступен\n",
        "\n",
        "    # Примеры текстов для тестирования\n",
        "    pairs = [\n",
        "        (\n",
        "            \"На перекрестке Монтажников и Расточная три машины не поделили дорогу.\",\n",
        "            \"ДТП на Монтажников и Расточная. Дорогу не поделили, затор на час.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Свалка мусора на улице Кирова. Нужно убирать.\",\n",
        "            \"На Кирова снова свалка! Уже неделю никто не убирает.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Яма на Ленина, 15. Машина повреждена.\",\n",
        "            \"Пробка на Монтажников и Расточная.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В Екатеринбурге вынесли приговор банде бывших полицейских.\",\n",
        "            \"Суд в центре Екатеринбурга вынес приговор.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\",\n",
        "            \"Лицей на день закрыл младшие классы для санобработки помещений.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\",\n",
        "            \"В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Светофор не работает на перекрестке 8 Марта - Куйбышева. Водители просят быть внимательнее.\",\n",
        "            \"В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В гимназии № 35 открылся новый IT-технопарк для уроков робототехники и программирования.\",\n",
        "            \"В нашей гимназии открыйти айти-технопарк! Лютая имба!\"\n",
        "        ),\n",
        "        (\n",
        "            \"В небо над городом запустят праздничный салют в честь Дня города.\",\n",
        "            \"В городе прошел традиционный осенний легкоатлетический кросс.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(f\"Тестирование пар текстов с порогом {TEXT_SIMILARITY_THRESHOLD}...\\n\")\n",
        "\n",
        "    for text1, text2 in pairs:\n",
        "        test_pair(text1, text2, model, TEXT_SIMILARITY_THRESHOLD)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mimaYWaWUrRW",
        "outputId": "e6d7d01a-ea60-449b-90a5-4127ed56e7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели...\n",
            "Тестирование пар текстов с порогом 0.5...\n",
            "\n",
            "Текст 1: На перекрестке Монтажников и Расточная три машины не поделили дорогу.\n",
            "Текст 2: ДТП на Монтажников и Расточная. Дорогу не поделили, затор на час.\n",
            "Схожесть: 0.6976\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: Свалка мусора на улице Кирова. Нужно убирать.\n",
            "Текст 2: На Кирова снова свалка! Уже неделю никто не убирает.\n",
            "Схожесть: 0.4935\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: Яма на Ленина, 15. Машина повреждена.\n",
            "Текст 2: Пробка на Монтажников и Расточная.\n",
            "Схожесть: 0.4813\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В Екатеринбурге вынесли приговор банде бывших полицейских.\n",
            "Текст 2: Суд в центре Екатеринбурга вынес приговор.\n",
            "Схожесть: 0.6820\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\n",
            "Текст 2: Лицей на день закрыл младшие классы для санобработки помещений.\n",
            "Схожесть: 0.6516\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\n",
            "Текст 2: В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\n",
            "Схожесть: 0.6038\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: Светофор не работает на перекрестке 8 Марта - Куйбышева. Водители просят быть внимательнее.\n",
            "Текст 2: В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\n",
            "Схожесть: 0.0912\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В гимназии № 35 открылся новый IT-технопарк для уроков робототехники и программирования.\n",
            "Текст 2: В нашей гимназии открыйти айти-технопарк! Лютая имба!\n",
            "Схожесть: 0.4864\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В небо над городом запустят праздничный салют в честь Дня города.\n",
            "Текст 2: В городе прошел традиционный осенний легкоатлетический кросс.\n",
            "Схожесть: 0.5799\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Отличный вариант с GigaChat и Spacy"
      ],
      "metadata": {
        "id": "mhDK07UpsBVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gigachat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juhz1BpEPm5g",
        "outputId": "2e28bcba-10d7-4525-f91d-5d0e14a18f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gigachat in /usr/local/lib/python3.12/dist-packages (0.1.43)\n",
            "Requirement already satisfied: httpx<1 in /usr/local/lib/python3.12/dist-packages (from gigachat) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=1 in /usr/local/lib/python3.12/dist-packages (from gigachat) (2.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1->gigachat) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Словарь топонимов Екатеринбурга\n",
        "TOPONYMS = [\n",
        "    \"Верх-Исетский\",\n",
        "    \"Железнодорожный\",\n",
        "    \"Кировский\",\n",
        "    \"Ленинский\",\n",
        "    \"Октябрьский\",\n",
        "    \"Орджоникидзевский\",\n",
        "    \"Чкаловский\",\n",
        "    \"Академический\",\n",
        "    \"Вторчермет\",\n",
        "    \"Втузгородок\",\n",
        "    \"Горный Щит\",\n",
        "    \"Елизаветинское\",\n",
        "    \"ЖБИ\",\n",
        "    \"Завокзальный\",\n",
        "    \"Изумрудный\",\n",
        "    \"Кольцово\",\n",
        "    \"Комсомольский\",\n",
        "    \"Короленковский\",\n",
        "    \"Малый Исток\",\n",
        "    \"Метеогорка\",\n",
        "    \"Нижнесвердловский\",\n",
        "    \"Новая Сортровка\",\n",
        "    \"Новобелореченский\",\n",
        "    \"Павелшина\",\n",
        "    \"Парковый\",\n",
        "    \"Пионерский\",\n",
        "    \"Психбольница\",\n",
        "    \"Рудничный\",\n",
        "    \"Семь Ключей\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Синие Камни\",\n",
        "    \"Сортировка\",\n",
        "    \"Старая Сортровка\",\n",
        "    \"Старый Октябрь\",\n",
        "    \"Татищева\",\n",
        "    \"Уктус\",\n",
        "    \"УНЦ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Центр\",\n",
        "    \"Черкасская\",\n",
        "    \"Шабровский\",\n",
        "    \"Шарташ\",\n",
        "    \"Широкая Речка\",\n",
        "    \"Эльмаш\",\n",
        "    \"Юго-Западный\",\n",
        "    \"8 Марта\",\n",
        "    \"Амундсена\",\n",
        "    \"Авиационная\",\n",
        "    \"Академика Павлова\",\n",
        "    \"Бабушкина\",\n",
        "    \"Бахчиванджи\",\n",
        "    \"Белинского\",\n",
        "    \"Бориса Ельцина\",\n",
        "    \"Братиславская\",\n",
        "    \"Бродова\",\n",
        "    \"Бульвар Академика Семихатова\",\n",
        "    \"Бутюлина\",\n",
        "    \"Валдайская\",\n",
        "    \"Верхняя Пышма\",\n",
        "    \"Вилонова\",\n",
        "    \"Водная\",\n",
        "    \"Волгоградская\",\n",
        "    \"Генеральская\",\n",
        "    \"Героев России\",\n",
        "    \"Глинки\",\n",
        "    \"Гоголя\",\n",
        "    \"Горького\",\n",
        "    \"Декабристов\",\n",
        "    \"Донбасская\",\n",
        "    \"Дружининская\",\n",
        "    \"Ереванская\",\n",
        "    \"Жуковского\",\n",
        "    \"Заводская\",\n",
        "    \"Зои Космодемьянской\",\n",
        "    \"Кирова\",\n",
        "    \"Колмогорова\",\n",
        "    \"Комсомольская\",\n",
        "    \"Короленко\",\n",
        "    \"Космонавтов\",\n",
        "    \"Крауля\",\n",
        "    \"Куйбышева\",\n",
        "    \"Лермонтова\",\n",
        "    \"Луначарского\",\n",
        "    \"Малышева\",\n",
        "    \"Мамина-Сибиряка\",\n",
        "    \"Маршала Жукова\",\n",
        "    \"Машинная\",\n",
        "    \"Мельникайте\",\n",
        "    \"Металлургов\",\n",
        "    \"Мира\",\n",
        "    \"Октябрьская\",\n",
        "    \"Павла Шпагина\",\n",
        "    \"Победы\",\n",
        "    \"Пролетарская\",\n",
        "    \"Проспект Космонавтов\",\n",
        "    \"Радищева\",\n",
        "    \"Репина\",\n",
        "    \"Розы Люксембург\",\n",
        "    \"Сакко и Ванцетти\",\n",
        "    \"Свердлова\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Софьи Ковалевской\",\n",
        "    \"Студенческая\",\n",
        "    \"Татищева\",\n",
        "    \"Техническая\",\n",
        "    \"Тургенева\",\n",
        "    \"Туполева\",\n",
        "    \"Учительская\",\n",
        "    \"Фрунзе\",\n",
        "    \"Чапаева\",\n",
        "    \"Чкалова\",\n",
        "    \"Шарташская\",\n",
        "    \"Шефская\",\n",
        "    \"Шиловская\",\n",
        "    \"Щорса\",\n",
        "    \"Энгельса\",\n",
        "    \"Площадь 1905 года\",\n",
        "    \"Площадь Труда\",\n",
        "    \"ЦУМ\",\n",
        "    \"Геологическая\",\n",
        "    \"ВИЗ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Сакко\",\n",
        "    \"Геолка\",\n",
        "    \"1905\",\n",
        "    \"Уралмашь\",\n",
        "    \"Выксунский завод\",\n",
        "    \"Ленина\",\n",
        "    \"Екатеринбург\",\n",
        "    \"Расточная\",\n",
        "    \"Монтажников\",\n",
        "    \"УрФУ\",\n",
        "    \"НВК\",\n",
        "    \"Новокольцовский\",\n",
        "    \"Кирова\",\n",
        "    \"Лицей\",\n",
        "    \"Детский\",\n",
        "    \"Детсад\",\n",
        "    \"Школа\",\n",
        "    \"Больница\",\n",
        "    \"Университет\"\n",
        "]"
      ],
      "metadata": {
        "id": "Y1ELi2XIaSYN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Первая итерация - простая кластеризация"
      ],
      "metadata": {
        "id": "ZJuIMioQaT63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_and_cluster_colab_spacy.py\n",
        "\n",
        "# Установка зависимостей (нужно запустить один раз)\n",
        "# pip install sentence-transformers scikit-learn rapidfuzz numpy spacy\n",
        "# python -m spacy download ru_core_news_sm\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import process\n",
        "import re\n",
        "\n",
        "from gigachat import GigaChat\n",
        "from gigachat.models import Chat, Messages, MessagesRole\n",
        "from google.colab import userdata\n",
        "\n",
        "# Пути к файлам\n",
        "MESSAGES_FILE = \"messages.json\"\n",
        "CLUSTERS_FILE = \"clusters_spacy.json\"\n",
        "\n",
        "# Название модели\n",
        "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Параметры\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.45\n",
        "EPS_DBSCAN = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "MIN_SAMPLES = 2\n",
        "GEOCODE_SIMILARITY_THRESHOLD = 0.5\n",
        "\n",
        "AUTH = userdata.get('SBER_AUTH')  # Получить в кабинете СберCloud\n",
        "# Инициализируем клиент (будем использовать в контексте)\n",
        "GIGA_MODEL = \"GigaChat-2\"  # или \"GigaChat-Pro\"\n",
        "\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s\\dа-яё]', ' ', text.lower())\n",
        "\n",
        "\n",
        "def extract_geo_with_gigachat(text: str, auth=AUTH, model=GIGA_MODEL) -> dict:\n",
        "    \"\"\"\n",
        "    Извлекает структурированную геоинформацию из текста с помощью GigaChat SDK.\n",
        "    Возвращает словарь в формате JSON.\n",
        "    \"\"\"\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"Ты — эксперт по анализу жалоб жителей. Твоя задача — извлечь из текста \"\n",
        "        \"географическую информацию (или похожую на такую) и вернуть её в виде строго структурированного JSON. \"\n",
        "        \"Не добавляй пояснений, не изменяй формат, не придумывай данные.\"\n",
        "    )\n",
        "\n",
        "    USER_PROMPT = f\"\"\"\n",
        "    Текст сообщения: \"{text[0:150]}\"\n",
        "    Извлеки следующие поля в формате JSON:\n",
        "    {{\n",
        "      \"geocode\": [\"улица Ленина, 45\", \"Советская ул., д.10\", \"школа №3\", \"Центральный район\", \"УрФУ\"]\n",
        "    }}\n",
        "\n",
        "    geocode - названия любых улиц, районов, зданий, мест или иных меток для идентификации положения.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        Messages(role=MessagesRole.SYSTEM, content=SYSTEM_PROMPT),\n",
        "        Messages(role=MessagesRole.USER, content=USER_PROMPT),\n",
        "    ]\n",
        "\n",
        "    payload = Chat(\n",
        "        messages=messages,\n",
        "        temperature=0.1,\n",
        "        max_tokens=300,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        with GigaChat(credentials=auth, model=model, verify_ssl_certs=False) as giga:\n",
        "            response = giga.chat(payload)\n",
        "            raw_text = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Очистка от возможных markdown-блоков\n",
        "            if raw_text.startswith(\"```\"):\n",
        "                raw_text = raw_text.split(\"```\")[1] if \"```\" in raw_text else raw_text\n",
        "            if raw_text.startswith(\"json\"):\n",
        "                raw_text = raw_text[4:].strip()\n",
        "\n",
        "            import json\n",
        "            return json.loads(raw_text)\n",
        "    except Exception as e:\n",
        "        print(f\"[GigaChat Geo] Ошибка при обработке текста '{text[:50]}...': {e}\")\n",
        "        return {\n",
        "            \"geocodes\": []\n",
        "        }\n",
        "\n",
        "\n",
        "def extract_geo_with_spacy(text: str) -> list[str]:\n",
        "    try:\n",
        "        import spacy\n",
        "\n",
        "        # Проверим, не пустой ли текст\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return []\n",
        "\n",
        "        nlp = spacy.load(\"ru_core_news_sm\")\n",
        "        doc = nlp(text)\n",
        "\n",
        "        geos = []\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in [\"LOC\", \"GPE\", \"FAC\"]:  # Location, Geopolitical Entity, Facility\n",
        "                geos.append(ent.text)\n",
        "        return geos\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при извлечении геокода через spacy: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_geocodes_enhanced(text: str) -> list[str]:\n",
        "    found = set()\n",
        "\n",
        "    # 2. Извлечение через NER (spacy)\n",
        "    ner_geos = extract_geo_with_spacy(text)\n",
        "    for geo in ner_geos:\n",
        "        # Сопоставляем с каноническим словарём\n",
        "        match, score, _ = process.extractOne(geo, TOPONYMS)\n",
        "        if score >= 50:\n",
        "          found.add(match)\n",
        "\n",
        "    # print(found)\n",
        "    # Если не смогли найти геокоды с spacy, вызываем GigaChat\n",
        "    if len(found) == 0:\n",
        "        found = extract_geo_with_gigachat(text)\n",
        "        return list(found[\"geocode\"])\n",
        "\n",
        "    return list(found)\n",
        "\n",
        "def geocode_similarity(g1: list[str], g2: list[str], threshold=GEOCODE_SIMILARITY_THRESHOLD) -> bool:\n",
        "    if not g1 or not g2:\n",
        "        return False\n",
        "    set1 = set(g1)\n",
        "    set2 = set(g2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    if union == 0:\n",
        "        return False\n",
        "    return (intersection / union) >= threshold\n",
        "\n",
        "def cluster_messages(messages, model):\n",
        "    # Добавляем geocode к каждому сообщению\n",
        "    for msg in messages:\n",
        "        msg[\"geocode\"] = extract_geocodes_enhanced(msg[\"text\"])\n",
        "\n",
        "    # Группируем сообщения по схожести geocode\n",
        "    clusters_by_geo = []\n",
        "    for msg in messages:\n",
        "        assigned = False\n",
        "        for geo_cluster in clusters_by_geo:\n",
        "            if geocode_similarity(msg[\"geocode\"], geo_cluster[0][\"geocode\"]):\n",
        "                geo_cluster.append(msg)\n",
        "                assigned = True\n",
        "                break\n",
        "        if not assigned:\n",
        "            clusters_by_geo.append([msg])\n",
        "\n",
        "    clusters = []\n",
        "    cluster_id_counter = 1\n",
        "\n",
        "    for geo_group in clusters_by_geo:\n",
        "        if len(geo_group) < 2:\n",
        "            continue\n",
        "\n",
        "        texts = [m[\"text\"] for m in geo_group]\n",
        "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "        clustering = DBSCAN(eps=EPS_DBSCAN, min_samples=MIN_SAMPLES, metric='cosine').fit(embeddings)\n",
        "\n",
        "        cluster_map = {}\n",
        "        for i, label in enumerate(clustering.labels_):\n",
        "            if label == -1:\n",
        "                continue\n",
        "            if label not in cluster_map:\n",
        "                cluster_map[label] = []\n",
        "            cluster_map[label].append(geo_group[i])\n",
        "\n",
        "        for cluster_label, cluster_msgs in cluster_map.items():\n",
        "            if len(cluster_msgs) < 2:\n",
        "                continue\n",
        "\n",
        "            primary_geo = cluster_msgs[0][\"geocode\"]\n",
        "            core_emb = np.mean(model.encode([m[\"text\"] for m in cluster_msgs], convert_to_numpy=True), axis=0)\n",
        "            cluster = {\n",
        "                \"cluster_id\": f\"cl_{cluster_id_counter:03d}\",\n",
        "                \"geocode\": primary_geo,\n",
        "                \"message_count\": len(cluster_msgs),\n",
        "                \"first_seen\": min(m[\"date\"] for m in cluster_msgs),\n",
        "                \"last_seen\": max(m[\"date\"] for m in cluster_msgs),\n",
        "                \"core_embedding\": core_emb.tolist(),\n",
        "                \"examples\": cluster_msgs\n",
        "            }\n",
        "            clusters.append(cluster)\n",
        "            cluster_id_counter += 1\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def main():\n",
        "    print(\"Загрузка модели L12...\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')\n",
        "\n",
        "    print(\"Загрузка сообщений...\")\n",
        "    with open(MESSAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        messages = json.load(f)\n",
        "\n",
        "    print(f\"Обнаружено {len(messages)} сообщений.\")\n",
        "\n",
        "    print(\"Извлечение геокодов и кластеризация...\")\n",
        "    clusters = cluster_messages(messages, model)\n",
        "\n",
        "    print(f\"Создано {len(clusters)} кластеров.\")\n",
        "\n",
        "    print(\"Сохранение кластеров...\")\n",
        "    with open(CLUSTERS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"clusters\": clusters}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Обработка завершена. Результат в {CLUSTERS_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # print(extract_geo_with_gigachat(\"Власти объявили о расширении Режевского тракта до 4 полос в следующем году.\"))\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45cZh6yLsVSO",
        "outputId": "8fdb3314-f1b7-42b6-b8c2-0698f392d493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели L12...\n",
            "Загрузка сообщений...\n",
            "Обнаружено 10 сообщений.\n",
            "Извлечение геокодов и кластеризация...\n",
            "set()\n",
            "{'Расточная'}\n",
            "{'Расточная'}\n",
            "set()\n",
            "set()\n",
            "{'Ленина'}\n",
            "{'Ленина'}\n",
            "{'Машинная'}\n",
            "set()\n",
            "{'Кирова'}\n",
            "Создано 2 кластеров.\n",
            "Сохранение кластеров...\n",
            "✅ Обработка завершена. Результат в clusters_spacy.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавляем id сообщениям"
      ],
      "metadata": {
        "id": "jTpaFw0QaYDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "# Путь к файлу\n",
        "INPUT_FILE = \"messages.json\"\n",
        "\n",
        "# Проверка существования\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    print(f\"❌ Файл {INPUT_FILE} не найден. Загрузите его в Colab.\")\n",
        "    exit()\n",
        "\n",
        "# Чтение\n",
        "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    messages = json.load(f)\n",
        "\n",
        "print(f\"📥 Загружено {len(messages)} сообщений.\")\n",
        "\n",
        "# Добавление ID\n",
        "updated = 0\n",
        "for msg in messages:\n",
        "    if \"id\" not in msg or not msg[\"id\"]:\n",
        "        # Используем URL как основу (лучший идентификатор для Telegram)\n",
        "        url = msg.get(\"url\", \"\").strip()\n",
        "        if url:\n",
        "            # Убираем пробелы и параметры после '?' (на всякий случай)\n",
        "            clean_url = url.split('?')[0].strip()\n",
        "            # Генерируем короткий хэш (16 символов, как в вашем примере)\n",
        "            msg_id = hashlib.sha256(clean_url.encode('utf-8')).hexdigest()[:16]\n",
        "        else:\n",
        "            # Fallback: хэш от текста + даты\n",
        "            fallback = msg.get(\"text\", \"\") + msg.get(\"date\", \"\")\n",
        "            msg_id = hashlib.md5(fallback.encode('utf-8')).hexdigest()[:16]\n",
        "\n",
        "        msg[\"id\"] = msg_id\n",
        "        updated += 1\n",
        "\n",
        "print(f\"🆕 Добавлено ID для {updated} сообщений.\")\n",
        "\n",
        "# Сохранение\n",
        "with open(INPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(messages, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Файл {INPUT_FILE} обновлён.\")"
      ],
      "metadata": {
        "id": "4lJB2MZGZ_0l",
        "outputId": "ffb94fb6-c9ef-42ac-b24a-b33d537f8697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Загружено 1 сообщений.\n",
            "🆕 Добавлено ID для 1 сообщений.\n",
            "✅ Файл messages.json обновлён.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вторая итерация - инкрементная кластеризация"
      ],
      "metadata": {
        "id": "zQVkaQJCaZ9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Важно: подгрузите перед началом топонимы из блоков выше\n",
        "\n",
        "А ещё у всех сообщений должен быть id. Чекните код выше, там можно сгенерировать их всем сообщениям в вашем датасете"
      ],
      "metadata": {
        "id": "pjTdA34fe1kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# УСТАНОВКА ЗАВИСИМОСТЕЙ (запустите один раз)\n",
        "!pip install -q sentence-transformers scikit-learn rapidfuzz spacy geopy gigachat\n",
        "\n",
        "# ЗАГРУЗКА МОДЕЛИ SPACY (один раз)\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"ru_core_news_sm\"])\n",
        "\n",
        "# ИМПОРТЫ\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from rapidfuzz import process\n",
        "import spacy\n",
        "\n",
        "# GigaChat\n",
        "from gigachat import GigaChat\n",
        "from gigachat.models import Chat, Messages, MessagesRole\n",
        "from google.colab import userdata\n",
        "\n",
        "# Geocoding\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "\n",
        "# ======================\n",
        "# КОНФИГУРАЦИЯ\n",
        "# ======================\n",
        "MESSAGES_FILE = \"messages.json\"\n",
        "CLUSTERS_FILE = \"clusters.json\"\n",
        "CACHE_DIR = \"cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "GEO_CACHE_FILE = os.path.join(CACHE_DIR, \"geocodes_cache.json\")\n",
        "PROCESSED_IDS_FILE = os.path.join(CACHE_DIR, \"processed_ids.json\")\n",
        "NOISE_BUFFER_FILE = os.path.join(CACHE_DIR, \"noise_buffer.json\")\n",
        "\n",
        "# Модель эмбеддингов\n",
        "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.45\n",
        "EPS_DBSCAN = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "MIN_SAMPLES = 2\n",
        "\n",
        "# TTL для буфера выбросов (часы)\n",
        "NOISE_TTL_HOURS = 24\n",
        "# Максимальный размер буфера выбросов\n",
        "MAX_NOISE_BUFFER_SIZE = 200\n",
        "\n",
        "# Геокодер (OpenStreetMap)\n",
        "geolocator = Nominatim(user_agent=\"ekb_municipal_ai_hackathon\")\n",
        "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
        "\n",
        "# Топонимы выше\n",
        "# .......\n",
        "\n",
        "# GigaChat\n",
        "AUTH = userdata.get('SBER_AUTH')\n",
        "GIGA_MODEL = \"GigaChat\"\n",
        "\n",
        "# ======================\n",
        "# ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ\n",
        "# ======================\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s\\dа-яё]', ' ', text.lower())\n",
        "\n",
        "def extract_geo_with_spacy(text: str) -> list[str]:\n",
        "    try:\n",
        "        nlp = spacy.load(\"ru_core_news_sm\")\n",
        "        doc = nlp(text)\n",
        "        return [ent.text for ent in doc.ents if ent.label_ in (\"LOC\", \"GPE\", \"FAC\")]\n",
        "    except Exception as e:\n",
        "        print(f\"[spaCy] Ошибка: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_geo_with_gigachat(text: str) -> list[str]:\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"Ты — эксперт по анализу жалоб жителей Екатеринбурга. \"\n",
        "        \"Извлеки из текста географические объекты (улицы, районы, здания, площади) \"\n",
        "        \"и верни ТОЛЬКО JSON-массив в поле 'geocode'.\"\n",
        "    )\n",
        "    USER_PROMPT = f\"\"\"\n",
        "      Текст сообщения: \"{text[:150]}\"\n",
        "      Примеры для твоих инструкций.\n",
        "      Вход: \"Вчера в бассейне 'Юность' произошла\"\n",
        "      Выход: {{\"geocode\": [\"Юность\", \"бассейн\"]}}\n",
        "      Вход: \"На перекрестке Монтажников - Расточная авария\",\n",
        "      Выход: {{\"geocode\": [\"Монтажников\", \"Расточная\"]}}\n",
        "      Вход: \"Ремонт на улице Монтажников затянулся.\"\n",
        "      Выход: {{\"geocode\": [\"Монтажников\"]}}\n",
        "      Вход: \"Дети из СОШ №3 жалуются на отопление.\"\n",
        "      Выход: {{\"geocode\": [\"школа №3\"]}}\n",
        "      Вход: \"Вчера в 'Юности' обедали\"\n",
        "      Выход: {{\"geocode\": [\"Юность\", \"обед\"]}}\n",
        "      Все формы (даже в кавычках) возвращай в именительном падеже\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        Messages(role=MessagesRole.SYSTEM, content=SYSTEM_PROMPT),\n",
        "        Messages(role=MessagesRole.USER, content=USER_PROMPT),\n",
        "    ]\n",
        "\n",
        "    payload = Chat(messages=messages, temperature=0.1, max_tokens=200)\n",
        "\n",
        "    try:\n",
        "        with GigaChat(credentials=AUTH, model=GIGA_MODEL, verify_ssl_certs=False) as giga:\n",
        "            resp = giga.chat(payload)\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "            if content.startswith(\"```\"):\n",
        "                content = content.split(\"```\")[1] if \"```\" in content else content\n",
        "            if content.startswith(\"json\"):\n",
        "                content = content[4:].strip()\n",
        "            data = json.loads(content)\n",
        "            return data.get(\"geocode\", [])\n",
        "    except Exception as e:\n",
        "        print(f\"[GigaChat] Ошибка: {e}\")\n",
        "        return []\n",
        "\n",
        "def geocode_toponyms(toponyms: list[str], city=\"Екатеринбург\") -> list[dict]:\n",
        "    results = []\n",
        "    for topo in toponyms:\n",
        "        query = f\"{topo}, {city}\"\n",
        "        try:\n",
        "            loc = geocode(query, country_codes=\"RU\", timeout=10)\n",
        "            if loc:\n",
        "                results.append({\n",
        "                    \"name\": topo,\n",
        "                    \"lat\": round(loc.latitude, 6),\n",
        "                    \"lon\": round(loc.longitude, 6)\n",
        "                })\n",
        "            else:\n",
        "                results.append({\"name\": topo, \"lat\": None, \"lon\": None})\n",
        "        except Exception as e:\n",
        "            print(f\"[Nominatim] {e}\")\n",
        "            results.append({\"name\": topo, \"lat\": None, \"lon\": None})\n",
        "    return results\n",
        "\n",
        "def extract_geocodes_cached(msg_id: str, text: str, geocache: dict) -> dict:\n",
        "    if msg_id in geocache:\n",
        "        return geocache[msg_id]\n",
        "\n",
        "    # 1. spaCy + словарь\n",
        "    found = set()\n",
        "    ner_geos = extract_geo_with_spacy(text)\n",
        "    for geo in ner_geos:\n",
        "        match, score, _ = process.extractOne(geo, TOPONYMS)\n",
        "        if score >= 50:\n",
        "            found.add(match)\n",
        "\n",
        "    source = \"spacy\"\n",
        "    if not found:\n",
        "        found = set(extract_geo_with_gigachat(text))\n",
        "        source = \"gigachat\"\n",
        "\n",
        "    found = list(found)\n",
        "    coords = geocode_toponyms(found) if found else []\n",
        "\n",
        "    result = {\n",
        "        \"geocode\": found,\n",
        "        \"coords\": coords,\n",
        "        \"source\": source,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    geocache[msg_id] = result\n",
        "    return result\n",
        "\n",
        "def load_cache():\n",
        "    geocache = {}\n",
        "    processed_ids = set()\n",
        "    if os.path.exists(GEO_CACHE_FILE):\n",
        "        with open(GEO_CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            geocache = json.load(f)\n",
        "    if os.path.exists(PROCESSED_IDS_FILE):\n",
        "        with open(PROCESSED_IDS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            processed_ids = set(json.load(f))\n",
        "    return geocache, processed_ids\n",
        "\n",
        "def save_cache(geocache, processed_ids):\n",
        "    with open(GEO_CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(geocache, f, ensure_ascii=False, indent=2)\n",
        "    with open(PROCESSED_IDS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(list(processed_ids), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_noise_buffer():\n",
        "    if os.path.exists(NOISE_BUFFER_FILE):\n",
        "        with open(NOISE_BUFFER_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            buffer = json.load(f)\n",
        "        now = datetime.now()\n",
        "        return [\n",
        "            msg for msg in buffer\n",
        "            if (now - datetime.fromisoformat(msg[\"buffered_at\"])) < timedelta(hours=NOISE_TTL_HOURS)\n",
        "        ]\n",
        "    return []\n",
        "\n",
        "def save_noise_buffer(buffer):\n",
        "    with open(NOISE_BUFFER_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(buffer, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def load_existing_clusters():\n",
        "    if not os.path.exists(CLUSTERS_FILE):\n",
        "        return []\n",
        "    try:\n",
        "        with open(CLUSTERS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        clusters = data.get(\"clusters\", [])\n",
        "        # Конвертируем эмбеддинги в numpy для сравнения\n",
        "        for cl in clusters:\n",
        "            if isinstance(cl[\"core_embedding\"], list):\n",
        "                cl[\"core_embedding\"] = np.array(cl[\"core_embedding\"], dtype=np.float32)\n",
        "        return clusters\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Не удалось загрузить кластеры: {e}\")\n",
        "        return []\n",
        "\n",
        "def save_clusters(clusters):\n",
        "    # Преобразуем эмбеддинги обратно в списки для JSON\n",
        "    serializable = []\n",
        "    for cl in clusters:\n",
        "        cl_copy = cl.copy()\n",
        "        if isinstance(cl_copy[\"core_embedding\"], np.ndarray):\n",
        "            cl_copy[\"core_embedding\"] = cl_copy[\"core_embedding\"].tolist()\n",
        "        serializable.append(cl_copy)\n",
        "    with open(CLUSTERS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"clusters\": serializable}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def update_cluster_with_message(cluster, new_msg, model, max_examples=5):\n",
        "    # Собираем все тексты: старые примеры + новое сообщение\n",
        "    all_texts = [ex[\"text\"] for ex in cluster[\"examples\"]] + [new_msg[\"text\"]]\n",
        "    new_emb = np.mean(model.encode(all_texts, convert_to_numpy=True), axis=0)\n",
        "\n",
        "    # Обновляем примеры — оставляем не более max_examples, приоритет свежим\n",
        "    updated_examples = (cluster[\"examples\"] + [new_msg])[-max_examples:]\n",
        "\n",
        "    cluster.update({\n",
        "        \"core_embedding\": new_emb,\n",
        "        \"message_count\": cluster[\"message_count\"] + 1,\n",
        "        \"last_seen\": max(cluster[\"last_seen\"], new_msg[\"date\"]),\n",
        "        \"examples\": updated_examples\n",
        "    })\n",
        "\n",
        "def try_assign_to_existing_clusters(new_messages, existing_clusters, model, geo_threshold=0.5, sim_threshold=0.45):\n",
        "    unassigned = []\n",
        "    for msg in new_messages:\n",
        "        assigned = False\n",
        "        msg_emb = model.encode([msg[\"text\"]], convert_to_numpy=True)[0]\n",
        "\n",
        "        for cl in existing_clusters:\n",
        "            # Географическое совпадение (твоя функция, без изменений)\n",
        "            if not geocode_similarity(msg[\"geocode\"], cl[\"geocode\"], threshold=geo_threshold):\n",
        "                continue\n",
        "\n",
        "            # Семантическое сходство\n",
        "            sim = cosine_similarity([msg_emb], [cl[\"core_embedding\"]])[0][0]\n",
        "            if sim >= sim_threshold:\n",
        "                update_cluster_with_message(cl, msg, model)\n",
        "                assigned = True\n",
        "                break\n",
        "\n",
        "        if not assigned:\n",
        "            unassigned.append(msg)\n",
        "    return unassigned\n",
        "\n",
        "# ======================\n",
        "# КЛАСТЕРИЗАЦИЯ\n",
        "# ======================\n",
        "\n",
        "def geocode_similarity(g1, g2, threshold=0.5):\n",
        "    if not g1 or not g2:\n",
        "        return False\n",
        "    s1, s2 = set(g1), set(g2)\n",
        "    return len(set(s1 & s2)) / len(set(s1 | s2)) >= threshold\n",
        "\n",
        "def cluster_with_geo_grouping(messages, model):\n",
        "    # Геогруппировка\n",
        "    geo_groups = []\n",
        "    for msg in messages:\n",
        "        placed = False\n",
        "        for group in geo_groups:\n",
        "            if geocode_similarity(msg[\"geocode\"], group[0][\"geocode\"]):\n",
        "                group.append(msg)\n",
        "                placed = True\n",
        "                break\n",
        "        if not placed:\n",
        "            geo_groups.append([msg])\n",
        "\n",
        "    clusters, noise = [], []\n",
        "    for group in geo_groups:\n",
        "        if len(group) == 1:\n",
        "            noise.extend(group)\n",
        "            continue\n",
        "\n",
        "        # Семантическая кластеризация внутри геогруппы\n",
        "        texts = [m[\"text\"] for m in group]\n",
        "        emb = model.encode(texts, convert_to_numpy=True)\n",
        "        labels = DBSCAN(eps=EPS_DBSCAN, min_samples=MIN_SAMPLES, metric='cosine').fit(emb).labels_\n",
        "\n",
        "        clustered_ids = set()\n",
        "        for label in set(labels):\n",
        "            if label == -1:\n",
        "                continue\n",
        "            msgs = [group[i] for i, l in enumerate(labels) if l == label]\n",
        "            if len(msgs) >= 2:\n",
        "                core_emb = np.mean(model.encode([m[\"text\"] for m in msgs], convert_to_numpy=True), axis=0)\n",
        "                clusters.append({\n",
        "                    \"cluster_id\": f\"cl_{len(clusters)+1:03d}\",\n",
        "                    \"geocode\": msgs[0][\"geocode\"],\n",
        "                    \"coords\": msgs[0][\"coords\"],\n",
        "                    \"message_count\": len(msgs),\n",
        "                    \"first_seen\": min(m[\"date\"] for m in msgs),\n",
        "                    \"last_seen\": max(m[\"date\"] for m in msgs),\n",
        "                    \"core_embedding\": core_emb.tolist(),\n",
        "                    \"examples\": msgs\n",
        "                })\n",
        "                clustered_ids.update(m[\"id\"] for m in msgs)\n",
        "\n",
        "        # Не вошедшие в кластеры — в шум\n",
        "        noise.extend([m for m in group if m[\"id\"] not in clustered_ids])\n",
        "\n",
        "    return clusters, noise\n",
        "\n",
        "# ======================\n",
        "# MAIN\n",
        "# ======================\n",
        "\n",
        "def main():\n",
        "    print(\"🚀 Запуск AI-агента для Главы Екатеринбурга...\")\n",
        "    with open(MESSAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        messages = json.load(f)\n",
        "    if not messages:\n",
        "        return\n",
        "\n",
        "    # Загрузка модели\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')\n",
        "\n",
        "    # Кэш обработанных сообщений\n",
        "    geocache, processed_ids = load_cache()\n",
        "    new_messages = [m for m in messages if m[\"id\"] not in processed_ids]\n",
        "    print(f\"🆕 Новых сообщений: {len(new_messages)}\")\n",
        "\n",
        "    if not new_messages:\n",
        "        print(\"💤 Нет новых данных.\")\n",
        "        return\n",
        "\n",
        "    # Геокодирование новых сообщений\n",
        "    for msg in new_messages:\n",
        "        geo_data = extract_geocodes_cached(msg[\"id\"], msg[\"text\"], geocache)\n",
        "        msg[\"geocode\"] = geo_data[\"geocode\"]\n",
        "        msg[\"coords\"] = geo_data[\"coords\"]\n",
        "\n",
        "    for msg in new_messages:\n",
        "        processed_ids.add(msg[\"id\"])\n",
        "    save_cache(geocache, processed_ids)\n",
        "\n",
        "    # === ИНКРЕМЕНТАЛЬНОЕ ОБНОВЛЕНИЕ КЛАСТЕРОВ ===\n",
        "    # 1. Загружаем существующие кластеры\n",
        "    existing_clusters = load_existing_clusters()\n",
        "\n",
        "    # 2. Пробуем присоединить новые сообщения к существующим кластерам\n",
        "    remaining_new = try_assign_to_existing_clusters(\n",
        "        new_messages,\n",
        "        existing_clusters,\n",
        "        model,\n",
        "        geo_threshold=0.3,\n",
        "        sim_threshold=TEXT_SIMILARITY_THRESHOLD  # используем твой порог\n",
        "    )\n",
        "\n",
        "    # 3. Загружаем буфер выбросов и объединяем с неприсоединёнными новыми\n",
        "    noise_buffer = load_noise_buffer()\n",
        "    all_for_clustering = remaining_new + noise_buffer\n",
        "\n",
        "    # 4. Кластеризуем только то, что не попало в старые кластеры\n",
        "    new_clusters, remaining_noise = cluster_with_geo_grouping(all_for_clustering, model)\n",
        "\n",
        "    # 5. Объединяем обновлённые старые + новые кластеры\n",
        "    all_clusters = existing_clusters + new_clusters\n",
        "\n",
        "    # 6. Обновляем буфер выбросов\n",
        "    now = datetime.now().isoformat()\n",
        "    for msg in remaining_noise:\n",
        "        msg[\"buffered_at\"] = now\n",
        "    save_noise_buffer(remaining_noise)\n",
        "\n",
        "    # 7. Сохраняем ВСЕ кластеры (старые + новые)\n",
        "    save_clusters(all_clusters)\n",
        "\n",
        "    print(f\"\\n✅ Готово!\")\n",
        "    print(f\"  • Обновлено/создано кластеров: {len(all_clusters)}\")\n",
        "    print(f\"  • В буфере: {len(remaining_noise)} (TTL={NOISE_TTL_HOURS}ч)\")\n",
        "    print(f\"  • Результат: {CLUSTERS_FILE}\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5smFXAsUd01t",
        "outputId": "79055433-aa42-416a-d2c7-7d3c18c4a4c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Запуск AI-агента для Главы Екатеринбурга...\n",
            "🆕 Новых сообщений: 1\n",
            "\n",
            "✅ Готово!\n",
            "  • Обновлено/создано кластеров: 13\n",
            "  • В буфере: 45 (TTL=24ч)\n",
            "  • Результат: clusters.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_0YJgD1OXe62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестируем GigaChat на географию"
      ],
      "metadata": {
        "id": "Fx8EmGB9Xgfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# УСТАНОВКА ЗАВИСИМОСТЕЙ (запустите один раз)\n",
        "!pip install gigachat"
      ],
      "metadata": {
        "id": "rJEF78KRYYUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GigaChat\n",
        "import json\n",
        "from gigachat import GigaChat\n",
        "from gigachat.models import Chat, Messages, MessagesRole\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Топонимы выше\n",
        "# .......\n",
        "\n",
        "# GigaChat\n",
        "AUTH = userdata.get('SBER_AUTH')\n",
        "GIGA_MODEL = \"GigaChat-2\"\n",
        "\n",
        "def extract_geo_with_gigachat(text: str) -> list[str]:\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"Ты — эксперт по анализу жалоб жителей Екатеринбурга. \"\n",
        "        \"Извлеки из текста географические объекты (улицы, районы, здания, площади) \"\n",
        "        \"и верни ТОЛЬКО JSON-массив в поле 'geocode'.\"\n",
        "    )\n",
        "    USER_PROMPT = f\"\"\"\n",
        "      Текст сообщения: \"{text[:150]}\"\n",
        "      Примеры для твоих инструкций.\n",
        "      Вход: \"Вчера в бассейне 'Юность' произошла\"\n",
        "      Выход: {{\"geocode\": [\"Юность\", \"бассейн\"]}}\n",
        "      Вход: \"На перекрестке Монтажников - Расточная авария\",\n",
        "      Выход: {{\"geocode\": [\"Монтажников\", \"Расточная\"]}}\n",
        "      Вход: \"Ремонт на улице Монтажников затянулся.\"\n",
        "      Выход: {{\"geocode\": [\"Монтажников\"]}}\n",
        "      Вход: \"Дети из СОШ №3 жалуются на отопление.\"\n",
        "      Выход: {{\"geocode\": [\"школа №3\"]}}\n",
        "      Вход: \"Вчера в 'Юности' обедали\"\n",
        "      Выход: {{\"geocode\": [\"Юность\", \"обед\"]}}\n",
        "      Все формы (даже в кавычках) возвращай в именительном падеже. Игнорируй названия стран, городов\n",
        "    \"\"\"\n",
        "    print(USER_PROMPT)\n",
        "\n",
        "    messages = [\n",
        "        Messages(role=MessagesRole.SYSTEM, content=SYSTEM_PROMPT),\n",
        "        Messages(role=MessagesRole.USER, content=USER_PROMPT),\n",
        "    ]\n",
        "\n",
        "    payload = Chat(messages=messages, temperature=0.1, max_tokens=200)\n",
        "\n",
        "    try:\n",
        "        with GigaChat(credentials=AUTH, model=GIGA_MODEL, verify_ssl_certs=False) as giga:\n",
        "            resp = giga.chat(payload)\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "            if content.startswith(\"```\"):\n",
        "                content = content.split(\"```\")[1] if \"```\" in content else content\n",
        "            if content.startswith(\"json\"):\n",
        "                content = content[4:].strip()\n",
        "            data = json.loads(content)\n",
        "            return data.get(\"geocode\", [])\n",
        "    except Exception as e:\n",
        "        print(f\"[GigaChat] Ошибка: {e}\")\n",
        "        return []\n",
        "\n",
        "print(extract_geo_with_gigachat(\"На линии Екатеринбург - Первоуральск начал курсировать обновленный состав.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTPYh79FXkX2",
        "outputId": "ecbb5c26-be21-4393-ee05-ea6a7085f5f8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Текст сообщения: \"На линии Екатеринбург - Первоуральск начал курсировать обновленный состав.\"\n",
            "      Примеры для твоих инструкций.\n",
            "      Вход: \"Вчера в бассейне 'Юность' произошла\"\n",
            "      Выход: {\"geocode\": [\"Юность\", \"бассейн\"]}\n",
            "      Вход: \"На перекрестке Монтажников - Расточная авария\",\n",
            "      Выход: {\"geocode\": [\"Монтажников\", \"Расточная\"]}\n",
            "      Вход: \"Ремонт на улице Монтажников затянулся.\"\n",
            "      Выход: {\"geocode\": [\"Монтажников\"]}\n",
            "      Вход: \"Дети из СОШ №3 жалуются на отопление.\"\n",
            "      Выход: {\"geocode\": [\"школа №3\"]}\n",
            "      Вход: \"Вчера в 'Юности' обедали\"\n",
            "      Выход: {\"geocode\": [\"Юность\", \"обед\"]}\n",
            "      Все формы (даже в кавычках) возвращай в именительном падеже.\n",
            "    \n",
            "['Екатеринбург', 'Первоуральск']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XPT-x4txXkE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "me = {1, 2}\n",
        "me2 = {1, 3}\n",
        "print(me | me2)"
      ],
      "metadata": {
        "id": "KVlni_H4sA4i",
        "outputId": "b522493d-ea5d-41e4-b987-daa090498e7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1, 2, 3}\n"
          ]
        }
      ]
    }
  ]
}