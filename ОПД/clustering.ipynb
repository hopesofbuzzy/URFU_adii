{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "53AgVEcWMmES"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hopesofbuzzy/URFU_adii/blob/main/%D0%9E%D0%9F%D0%94/clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Средний вариант с fuzzy-search"
      ],
      "metadata": {
        "id": "53AgVEcWMmES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "vKm20zAXPS3r",
        "outputId": "dde89fa0-89c0-46f9-e890-0f6d7bbdc971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (3.14.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_and_cluster_colab_L12.py\n",
        "\n",
        "# Установка зависимостей (нужно запустить один раз)\n",
        "# pip install sentence-transformers scikit-learn rapidfuzz numpy\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import process\n",
        "import re\n",
        "\n",
        "# Пути к файлам (в Colab файлы лежат в корне по умолчанию)\n",
        "MESSAGES_FILE = \"messages.json\"\n",
        "CLUSTERS_FILE = \"clusters_L12.json\"\n",
        "\n",
        "# Название модели (загрузится автоматически)\n",
        "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Параметры\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.65\n",
        "EPS_DBSCAN = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "MIN_SAMPLES = 2\n",
        "GEOCODE_SIMILARITY_THRESHOLD = 0.1\n",
        "\n",
        "# Словарь топонимов Екатеринбурга\n",
        "# Словарь топонимов Екатеринбурга\n",
        "# Словарь топонимов Екатеринбурга\n",
        "TOPONYMS = [\n",
        "    \"Верх-Исетский\",\n",
        "    \"Железнодорожный\",\n",
        "    \"Кировский\",\n",
        "    \"Ленинский\",\n",
        "    \"Октябрьский\",\n",
        "    \"Орджоникидзевский\",\n",
        "    \"Чкаловский\",\n",
        "    \"Академический\",\n",
        "    \"Вторчермет\",\n",
        "    \"Втузгородок\",\n",
        "    \"Горный Щит\",\n",
        "    \"Елизаветинское\",\n",
        "    \"ЖБИ\",\n",
        "    \"Завокзальный\",\n",
        "    \"Изумрудный\",\n",
        "    \"Кольцово\",\n",
        "    \"Комсомольский\",\n",
        "    \"Короленковский\",\n",
        "    \"Малый Исток\",\n",
        "    \"Метеогорка\",\n",
        "    \"Нижнесвердловский\",\n",
        "    \"Новая Сортровка\",\n",
        "    \"Новобелореченский\",\n",
        "    \"Павелшина\",\n",
        "    \"Парковый\",\n",
        "    \"Пионерский\",\n",
        "    \"Психбольница\",\n",
        "    \"Рудничный\",\n",
        "    \"Семь Ключей\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Синие Камни\",\n",
        "    \"Сортировка\",\n",
        "    \"Старая Сортровка\",\n",
        "    \"Старый Октябрь\",\n",
        "    \"Татищева\",\n",
        "    \"Уктус\",\n",
        "    \"УНЦ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Центр\",\n",
        "    \"Черкасская\",\n",
        "    \"Шабровский\",\n",
        "    \"Шарташ\",\n",
        "    \"Широкая Речка\",\n",
        "    \"Эльмаш\",\n",
        "    \"Юго-Западный\",\n",
        "    \"8 Марта\",\n",
        "    \"Амундсена\",\n",
        "    \"Авиационная\",\n",
        "    \"Академика Павлова\",\n",
        "    \"Бабушкина\",\n",
        "    \"Бахчиванджи\",\n",
        "    \"Белинского\",\n",
        "    \"Бориса Ельцина\",\n",
        "    \"Братиславская\",\n",
        "    \"Бродова\",\n",
        "    \"Бульвар Академика Семихатова\",\n",
        "    \"Бутюлина\",\n",
        "    \"Валдайская\",\n",
        "    \"Верхняя Пышма\",\n",
        "    \"Вилонова\",\n",
        "    \"Водная\",\n",
        "    \"Волгоградская\",\n",
        "    \"Генеральская\",\n",
        "    \"Героев России\",\n",
        "    \"Глинки\",\n",
        "    \"Гоголя\",\n",
        "    \"Горького\",\n",
        "    \"Декабристов\",\n",
        "    \"Донбасская\",\n",
        "    \"Дружининская\",\n",
        "    \"Ереванская\",\n",
        "    \"Жуковского\",\n",
        "    \"Заводская\",\n",
        "    \"Зои Космодемьянской\",\n",
        "    \"Кирова\",\n",
        "    \"Колмогорова\",\n",
        "    \"Комсомольская\",\n",
        "    \"Короленко\",\n",
        "    \"Космонавтов\",\n",
        "    \"Крауля\",\n",
        "    \"Куйбышева\",\n",
        "    \"Лермонтова\",\n",
        "    \"Луначарского\",\n",
        "    \"Малышева\",\n",
        "    \"Мамина-Сибиряка\",\n",
        "    \"Маршала Жукова\",\n",
        "    \"Машинная\",\n",
        "    \"Мельникайте\",\n",
        "    \"Металлургов\",\n",
        "    \"Мира\",\n",
        "    \"Октябрьская\",\n",
        "    \"Павла Шпагина\",\n",
        "    \"Победы\",\n",
        "    \"Пролетарская\",\n",
        "    \"Проспект Космонавтов\",\n",
        "    \"Радищева\",\n",
        "    \"Репина\",\n",
        "    \"Розы Люксембург\",\n",
        "    \"Сакко и Ванцетти\",\n",
        "    \"Свердлова\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Софьи Ковалевской\",\n",
        "    \"Студенческая\",\n",
        "    \"Татищева\",\n",
        "    \"Техническая\",\n",
        "    \"Тургенева\",\n",
        "    \"Туполева\",\n",
        "    \"Учительская\",\n",
        "    \"Фрунзе\",\n",
        "    \"Чапаева\",\n",
        "    \"Чкалова\",\n",
        "    \"Шарташская\",\n",
        "    \"Шефская\",\n",
        "    \"Шиловская\",\n",
        "    \"Щорса\",\n",
        "    \"Энгельса\",\n",
        "    \"Площадь 1905 года\",\n",
        "    \"Площадь Труда\",\n",
        "    \"ЦУМ\",\n",
        "    \"Геологическая\",\n",
        "    \"ВИЗ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Сакко\",\n",
        "    \"Геолка\",\n",
        "    \"1905\",\n",
        "    \"Уралмашь\",\n",
        "    \"Выксунский завод\",\n",
        "    \"Монтажников\",\n",
        "    \"Расточная\",\n",
        "    \"Екатеринбург\",\n",
        "    \"Расточная\",\n",
        "    \"Монтажников\",\n",
        "    \"УрФУ\",\n",
        "    \"НВК\",\n",
        "    \"Новокольцовский\",\n",
        "    \"НВК\"\n",
        "]\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s\\dа-яё]', ' ', text.lower())\n",
        "\n",
        "def extract_geocodes(text: str) -> list[str]:\n",
        "    text = normalize_text(text)\n",
        "    found = set()\n",
        "    for word in text.split():\n",
        "        match, score, _ = process.extractOne(word, TOPONYMS)\n",
        "        if score >= 75:\n",
        "            found.add(match)\n",
        "    return list(found)\n",
        "\n",
        "def geocode_similarity(g1: list[str], g2: list[str], threshold=GEOCODE_SIMILARITY_THRESHOLD) -> bool:\n",
        "    if not g1 or not g2:\n",
        "        return False\n",
        "    set1 = set(g1)\n",
        "    set2 = set(g2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    if union == 0:\n",
        "        return False\n",
        "    return (intersection / union) >= threshold\n",
        "\n",
        "def cluster_messages(messages, model):\n",
        "    # Добавляем geocode к каждому сообщению\n",
        "    for msg in messages:\n",
        "        msg[\"geocode\"] = extract_geocodes(msg[\"text\"])\n",
        "\n",
        "    # Группируем сообщения по схожести geocode\n",
        "    clusters_by_geo = []\n",
        "    for msg in messages:\n",
        "        assigned = False\n",
        "        for geo_cluster in clusters_by_geo:\n",
        "            if geocode_similarity(msg[\"geocode\"], geo_cluster[0][\"geocode\"]):\n",
        "                geo_cluster.append(msg)\n",
        "                assigned = True\n",
        "                break\n",
        "        if not assigned:\n",
        "            clusters_by_geo.append([msg])\n",
        "\n",
        "    clusters = []\n",
        "    cluster_id_counter = 1\n",
        "\n",
        "    for geo_group in clusters_by_geo:\n",
        "        if len(geo_group) < 2:\n",
        "            continue\n",
        "\n",
        "        texts = [m[\"text\"] for m in geo_group]\n",
        "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "        clustering = DBSCAN(eps=EPS_DBSCAN, min_samples=MIN_SAMPLES, metric='cosine').fit(embeddings)\n",
        "\n",
        "        cluster_map = {}\n",
        "        for i, label in enumerate(clustering.labels_):\n",
        "            if label == -1:\n",
        "                continue\n",
        "            if label not in cluster_map:\n",
        "                cluster_map[label] = []\n",
        "            cluster_map[label].append(geo_group[i])\n",
        "\n",
        "        for cluster_label, cluster_msgs in cluster_map.items():\n",
        "            if len(cluster_msgs) < 2:\n",
        "                continue\n",
        "\n",
        "            primary_geo = cluster_msgs[0][\"geocode\"]\n",
        "            core_emb = np.mean(model.encode([m[\"text\"] for m in cluster_msgs], convert_to_numpy=True), axis=0)\n",
        "            cluster = {\n",
        "                \"cluster_id\": f\"cl_{cluster_id_counter:03d}\",\n",
        "                \"geocode\": primary_geo,\n",
        "                \"message_count\": len(cluster_msgs),\n",
        "                \"first_seen\": min(m[\"date\"] for m in cluster_msgs),\n",
        "                \"last_seen\": max(m[\"date\"] for m in cluster_msgs),\n",
        "                \"core_embedding\": core_emb.tolist(),\n",
        "                \"examples\": cluster_msgs\n",
        "            }\n",
        "            clusters.append(cluster)\n",
        "            cluster_id_counter += 1\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def main():\n",
        "    print(\"Загрузка модели L12...\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')  # Colab может использовать GPU, если доступно\n",
        "\n",
        "    print(\"Загрузка сообщений...\")\n",
        "    with open(MESSAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        messages = json.load(f)\n",
        "\n",
        "    print(f\"Обнаружено {len(messages)} сообщений.\")\n",
        "\n",
        "    print(\"Извлечение геокодов и кластеризация...\")\n",
        "    clusters = cluster_messages(messages, model)\n",
        "\n",
        "    print(f\"Создано {len(clusters)} кластеров.\")\n",
        "\n",
        "    print(\"Сохранение кластеров...\")\n",
        "    with open(CLUSTERS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"clusters\": clusters}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "from natasha import Doc, MorphVocab, NewsEmbedding, NewsNERTagger, NewsMorphTagger\n",
        "from razdel import sentenize, tokenize\n",
        "\n",
        "def extract_geo_with_natasha(text: str) -> list[str]:\n",
        "    doc = Doc(text)\n",
        "    embedding = NewsEmbedding()\n",
        "    morph_tagger = NewsMorphTagger(embedding)\n",
        "    ner_tagger = NewsNERTagger(embedding)\n",
        "\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    doc.tag_ner(ner_tagger)\n",
        "\n",
        "    geos = []\n",
        "    for span in doc.spans:\n",
        "        if span.type == 'LOC':  # или 'GPE' — Geopolitical Entity\n",
        "            geos.append(span.text)\n",
        "    return geos\n",
        "\n",
        "def extract_geocodes_enhanced(text: str) -> list[str]:\n",
        "    found = set()\n",
        "\n",
        "    # 1. Fuzzy-поиск по словарю\n",
        "    text_norm = normalize_text(text)\n",
        "    for word in text_norm.split():\n",
        "        match, score, _ = process.extractOne(word, TOPONYMS)\n",
        "        if score >= 85:\n",
        "            found.add(match)\n",
        "\n",
        "    # 2. Извлечение через NER (например, natasha)\n",
        "    ner_geos = extract_geo_with_natasha(text)\n",
        "    for geo in ner_geos:\n",
        "        # Сопоставляем с каноническим словарём\n",
        "        match, score, _ = process.extractOne(geo, TOPONYMS)\n",
        "        if score >= 80:\n",
        "            found.add(match)\n",
        "\n",
        "    # 3. Регулярки для паттернов\n",
        "    patterns = [\n",
        "        r'на\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"на Ленина\"\n",
        "        r'улица\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"улица Ленина\"\n",
        "        r'ул\\.\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"ул. Ленина\"\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        for m in matches:\n",
        "            match, score, _ = process.extractOne(m, TOPONYMS)\n",
        "            if score >= 85:\n",
        "                found.add(match)\n",
        "\n",
        "    return list(found)\n",
        "\n",
        "    print(f\"✅ Обработка завершена. Результат в {CLUSTERS_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  extract_geocodes_enhanced(\"В Кольцово объяснили, почему чемоданы пассажиров валялись на снегу.Как казалось, одна из телег наехала на снежный накат и выронила несколько чемоданов. Водитель автопоезда заметил падение багажа и вернул груз обратно.Отметим, что на взлетно-посадочную полосу багаж, ко\")\n",
        "    # main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "QXfnwF36cvrA",
        "outputId": "8444da05-188b-4978-989f-62c52beda9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2766330278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0mextract_geocodes_enhanced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"В Кольцово объяснили, почему чемоданы пассажиров валялись на снегу.Как казалось, одна из телег наехала на снежный накат и выронила несколько чемоданов. Водитель автопоезда заметил падение багажа и вернул груз обратно.Отметим, что на взлетно-посадочную полосу багаж, ко\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;31m# main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2766330278.py\u001b[0m in \u001b[0;36mextract_geocodes_enhanced\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# 2. Извлечение через NER (например, natasha)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mner_geos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_geo_with_natasha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgeo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mner_geos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Сопоставляем с каноническим словарём\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2766330278.py\u001b[0m in \u001b[0;36mextract_geo_with_natasha\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mner_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewsNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_tagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/natasha/doc.py\u001b[0m in \u001b[0;36mtag_morph\u001b[0;34m(self, tagger)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mtag_morph_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_syntax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/natasha/doc.py\u001b[0m in \u001b[0;36mtag_morph_doc\u001b[0;34m(doc, tagger)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtag_morph_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mmarkups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4hg6oRytNis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Отличный вариант с spacy\n"
      ],
      "metadata": {
        "id": "0qncwRwMtMSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers scikit-learn rapidfuzz numpy spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObBLZjzkwqMe",
        "outputId": "1896dd4d-7e34-4e4e-84b3-263e444f35b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.14.3\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_and_cluster_colab_spacy.py\n",
        "\n",
        "# Установка зависимостей (нужно запустить один раз)\n",
        "# pip install sentence-transformers scikit-learn rapidfuzz numpy spacy\n",
        "# python -m spacy download ru_core_news_sm\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import process\n",
        "import re\n",
        "\n",
        "# Пути к файлам\n",
        "MESSAGES_FILE = \"messages.json\"\n",
        "CLUSTERS_FILE = \"clusters_spacy.json\"\n",
        "\n",
        "# Название модели\n",
        "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Параметры\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.55\n",
        "EPS_DBSCAN = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "MIN_SAMPLES = 2\n",
        "GEOCODE_SIMILARITY_THRESHOLD = 0.5\n",
        "\n",
        "# Словарь топонимов Екатеринбурга\n",
        "TOPONYMS = [\n",
        "    \"Верх-Исетский\",\n",
        "    \"Железнодорожный\",\n",
        "    \"Кировский\",\n",
        "    \"Ленинский\",\n",
        "    \"Октябрьский\",\n",
        "    \"Орджоникидзевский\",\n",
        "    \"Чкаловский\",\n",
        "    \"Академический\",\n",
        "    \"Вторчермет\",\n",
        "    \"Втузгородок\",\n",
        "    \"Горный Щит\",\n",
        "    \"Елизаветинское\",\n",
        "    \"ЖБИ\",\n",
        "    \"Завокзальный\",\n",
        "    \"Изумрудный\",\n",
        "    \"Кольцово\",\n",
        "    \"Комсомольский\",\n",
        "    \"Короленковский\",\n",
        "    \"Малый Исток\",\n",
        "    \"Метеогорка\",\n",
        "    \"Нижнесвердловский\",\n",
        "    \"Новая Сортровка\",\n",
        "    \"Новобелореченский\",\n",
        "    \"Павелшина\",\n",
        "    \"Парковый\",\n",
        "    \"Пионерский\",\n",
        "    \"Психбольница\",\n",
        "    \"Рудничный\",\n",
        "    \"Семь Ключей\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Синие Камни\",\n",
        "    \"Сортировка\",\n",
        "    \"Старая Сортровка\",\n",
        "    \"Старый Октябрь\",\n",
        "    \"Татищева\",\n",
        "    \"Уктус\",\n",
        "    \"УНЦ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Центр\",\n",
        "    \"Черкасская\",\n",
        "    \"Шабровский\",\n",
        "    \"Шарташ\",\n",
        "    \"Широкая Речка\",\n",
        "    \"Эльмаш\",\n",
        "    \"Юго-Западный\",\n",
        "    \"8 Марта\",\n",
        "    \"Амундсена\",\n",
        "    \"Авиационная\",\n",
        "    \"Академика Павлова\",\n",
        "    \"Бабушкина\",\n",
        "    \"Бахчиванджи\",\n",
        "    \"Белинского\",\n",
        "    \"Бориса Ельцина\",\n",
        "    \"Братиславская\",\n",
        "    \"Бродова\",\n",
        "    \"Бульвар Академика Семихатова\",\n",
        "    \"Бутюлина\",\n",
        "    \"Валдайская\",\n",
        "    \"Верхняя Пышма\",\n",
        "    \"Вилонова\",\n",
        "    \"Водная\",\n",
        "    \"Волгоградская\",\n",
        "    \"Генеральская\",\n",
        "    \"Героев России\",\n",
        "    \"Глинки\",\n",
        "    \"Гоголя\",\n",
        "    \"Горького\",\n",
        "    \"Декабристов\",\n",
        "    \"Донбасская\",\n",
        "    \"Дружининская\",\n",
        "    \"Ереванская\",\n",
        "    \"Жуковского\",\n",
        "    \"Заводская\",\n",
        "    \"Зои Космодемьянской\",\n",
        "    \"Кирова\",\n",
        "    \"Колмогорова\",\n",
        "    \"Комсомольская\",\n",
        "    \"Короленко\",\n",
        "    \"Космонавтов\",\n",
        "    \"Крауля\",\n",
        "    \"Куйбышева\",\n",
        "    \"Лермонтова\",\n",
        "    \"Луначарского\",\n",
        "    \"Малышева\",\n",
        "    \"Мамина-Сибиряка\",\n",
        "    \"Маршала Жукова\",\n",
        "    \"Машинная\",\n",
        "    \"Мельникайте\",\n",
        "    \"Металлургов\",\n",
        "    \"Мира\",\n",
        "    \"Октябрьская\",\n",
        "    \"Павла Шпагина\",\n",
        "    \"Победы\",\n",
        "    \"Пролетарская\",\n",
        "    \"Проспект Космонавтов\",\n",
        "    \"Радищева\",\n",
        "    \"Репина\",\n",
        "    \"Розы Люксембург\",\n",
        "    \"Сакко и Ванцетти\",\n",
        "    \"Свердлова\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Софьи Ковалевской\",\n",
        "    \"Студенческая\",\n",
        "    \"Татищева\",\n",
        "    \"Техническая\",\n",
        "    \"Тургенева\",\n",
        "    \"Туполева\",\n",
        "    \"Учительская\",\n",
        "    \"Фрунзе\",\n",
        "    \"Чапаева\",\n",
        "    \"Чкалова\",\n",
        "    \"Шарташская\",\n",
        "    \"Шефская\",\n",
        "    \"Шиловская\",\n",
        "    \"Щорса\",\n",
        "    \"Энгельса\",\n",
        "    \"Площадь 1905 года\",\n",
        "    \"Площадь Труда\",\n",
        "    \"ЦУМ\",\n",
        "    \"Геологическая\",\n",
        "    \"ВИЗ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Сакко\",\n",
        "    \"Геолка\",\n",
        "    \"1905\",\n",
        "    \"Уралмашь\",\n",
        "    \"Выксунский завод\",\n",
        "    \"Ленина\",\n",
        "    \"Екатеринбург\",\n",
        "    \"Расточная\",\n",
        "    \"Монтажников\",\n",
        "    \"УрФУ\",\n",
        "    \"НВК\",\n",
        "    \"Новокольцовский\",\n",
        "    \"Кирова\",\n",
        "    \"Лицей\",\n",
        "    \"Детский\",\n",
        "    \"Детсад\",\n",
        "    \"Школа\",\n",
        "    \"Больница\",\n",
        "    \"Университет\"\n",
        "]\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s\\dа-яё]', ' ', text.lower())\n",
        "\n",
        "def extract_geo_with_spacy(text: str) -> list[str]:\n",
        "    try:\n",
        "        import spacy\n",
        "\n",
        "        # Проверим, не пустой ли текст\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return []\n",
        "\n",
        "        nlp = spacy.load(\"ru_core_news_sm\")\n",
        "        doc = nlp(text)\n",
        "\n",
        "        geos = []\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in [\"LOC\", \"GPE\", \"FAC\"]:  # Location, Geopolitical Entity, Facility\n",
        "                geos.append(ent.text)\n",
        "        return geos\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при извлечении геокода через spacy: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_geocodes_enhanced(text: str) -> list[str]:\n",
        "    found = set()\n",
        "\n",
        "    # # 1. Fuzzy-поиск по словарю\n",
        "    # text_norm = normalize_text(text)\n",
        "    # for word in text_norm.split():\n",
        "    #     match, score, _ = process.extractOne(word, TOPONYMS)\n",
        "    #     if score >= 85:\n",
        "    #         found.add(match)\n",
        "\n",
        "    # 2. Извлечение через NER (spacy)\n",
        "    ner_geos = extract_geo_with_spacy(text)\n",
        "    for geo in ner_geos:\n",
        "        # Сопоставляем с каноническим словарём\n",
        "        match, score, _ = process.extractOne(geo, TOPONYMS)\n",
        "        if score >= 70:\n",
        "          found.add(geo)\n",
        "\n",
        "    # # 3. Регулярки для паттернов\n",
        "    # patterns = [\n",
        "    #     r'на\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"на Ленина\"\n",
        "    #     r'улица\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"улица Ленина\"\n",
        "    #     r'ул\\.\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"ул. Ленина\"\n",
        "    # ]\n",
        "    # for pattern in patterns:\n",
        "    #     matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "    #     for m in matches:\n",
        "    #         match, score, _ = process.extractOne(m, TOPONYMS)\n",
        "    #         if score >= 85:\n",
        "    #             found.add(match)\n",
        "\n",
        "    return list(found)\n",
        "\n",
        "def geocode_similarity(g1: list[str], g2: list[str], threshold=GEOCODE_SIMILARITY_THRESHOLD) -> bool:\n",
        "    if not g1 or not g2:\n",
        "        return False\n",
        "    set1 = set(g1)\n",
        "    set2 = set(g2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    if union == 0:\n",
        "        return False\n",
        "    return (intersection / union) >= threshold\n",
        "\n",
        "def cluster_messages(messages, model):\n",
        "    # Добавляем geocode к каждому сообщению\n",
        "    for msg in messages:\n",
        "        msg[\"geocode\"] = extract_geocodes_enhanced(msg[\"text\"])\n",
        "\n",
        "    # Группируем сообщения по схожести geocode\n",
        "    clusters_by_geo = []\n",
        "    for msg in messages:\n",
        "        assigned = False\n",
        "        for geo_cluster in clusters_by_geo:\n",
        "            if geocode_similarity(msg[\"geocode\"], geo_cluster[0][\"geocode\"]):\n",
        "                geo_cluster.append(msg)\n",
        "                assigned = True\n",
        "                break\n",
        "        if not assigned:\n",
        "            clusters_by_geo.append([msg])\n",
        "\n",
        "    clusters = []\n",
        "    cluster_id_counter = 1\n",
        "\n",
        "    for geo_group in clusters_by_geo:\n",
        "        if len(geo_group) < 2:\n",
        "            continue\n",
        "\n",
        "        texts = [m[\"text\"] for m in geo_group]\n",
        "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "        clustering = DBSCAN(eps=EPS_DBSCAN, min_samples=MIN_SAMPLES, metric='cosine').fit(embeddings)\n",
        "\n",
        "        cluster_map = {}\n",
        "        for i, label in enumerate(clustering.labels_):\n",
        "            if label == -1:\n",
        "                continue\n",
        "            if label not in cluster_map:\n",
        "                cluster_map[label] = []\n",
        "            cluster_map[label].append(geo_group[i])\n",
        "\n",
        "        for cluster_label, cluster_msgs in cluster_map.items():\n",
        "            if len(cluster_msgs) < 2:\n",
        "                continue\n",
        "\n",
        "            primary_geo = cluster_msgs[0][\"geocode\"]\n",
        "            core_emb = np.mean(model.encode([m[\"text\"] for m in cluster_msgs], convert_to_numpy=True), axis=0)\n",
        "            cluster = {\n",
        "                \"cluster_id\": f\"cl_{cluster_id_counter:03d}\",\n",
        "                \"geocode\": primary_geo,\n",
        "                \"message_count\": len(cluster_msgs),\n",
        "                \"first_seen\": min(m[\"date\"] for m in cluster_msgs),\n",
        "                \"last_seen\": max(m[\"date\"] for m in cluster_msgs),\n",
        "                \"core_embedding\": core_emb.tolist(),\n",
        "                \"examples\": cluster_msgs\n",
        "            }\n",
        "            clusters.append(cluster)\n",
        "            cluster_id_counter += 1\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def main():\n",
        "    print(\"Загрузка модели L12...\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')\n",
        "\n",
        "    print(\"Загрузка сообщений...\")\n",
        "    with open(MESSAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        messages = json.load(f)\n",
        "\n",
        "    print(f\"Обнаружено {len(messages)} сообщений.\")\n",
        "\n",
        "    print(\"Извлечение геокодов и кластеризация...\")\n",
        "    clusters = cluster_messages(messages, model)\n",
        "\n",
        "    print(f\"Создано {len(clusters)} кластеров.\")\n",
        "\n",
        "    print(\"Сохранение кластеров...\")\n",
        "    with open(CLUSTERS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"clusters\": clusters}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Обработка завершена. Результат в {CLUSTERS_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(extract_geo_with_spacy(\"На Ленина пробки\"))\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "45cZh6yLsVSO",
        "outputId": "59edcb44-c632-4dc2-b085-813b851e5d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "Загрузка модели L12...\n",
            "Загрузка сообщений...\n",
            "Обнаружено 72 сообщений.\n",
            "Извлечение геокодов и кластеризация...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-2684141772.py\", line 320, in <cell line: 0>\n",
            "    main()\n",
            "  File \"/tmp/ipython-input-2684141772.py\", line 308, in main\n",
            "    clusters = cluster_messages(messages, model)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2684141772.py\", line 243, in cluster_messages\n",
            "    msg[\"geocode\"] = extract_geocodes_enhanced(msg[\"text\"])\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2684141772.py\", line 207, in extract_geocodes_enhanced\n",
            "    ner_geos = extract_geo_with_spacy(text)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2684141772.py\", line 184, in extract_geo_with_spacy\n",
            "    nlp = spacy.load(\"ru_core_news_sm\")\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\", line 52, in load\n",
            "    return util.load_model(\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/util.py\", line 524, in load_model\n",
            "    return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/util.py\", line 560, in load_model_from_package\n",
            "    return cls.load(vocab=vocab, disable=disable, enable=enable, exclude=exclude, config=config)  # type: ignore[attr-defined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ru_core_news_sm/__init__.py\", line 10, in load\n",
            "    return load_model_from_init_py(__file__, **overrides)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/util.py\", line 741, in load_model_from_init_py\n",
            "    return load_model_from_path(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/util.py\", line 606, in load_model_from_path\n",
            "    return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/language.py\", line 2245, in from_disk\n",
            "    util.from_disk(path, deserializers, exclude)  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/util.py\", line 1448, in from_disk\n",
            "    reader(path / key)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/language.py\", line 2221, in deserialize_vocab\n",
            "    self.vocab.from_disk(path, exclude=exclude)\n",
            "  File \"spacy/vocab.pyx\", line 553, in spacy.vocab.Vocab.from_disk\n",
            "  File \"spacy/vectors.pyx\", line 718, in spacy.vectors.Vectors.from_disk\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/util.py\", line 1439, in from_disk\n",
            "    def from_disk(\n",
            "    \n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1718, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
            "    file = getsourcefile(object) or getfile(object)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 970, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1009, in getmodule\n",
            "    if f == _filesbymodname.get(modname, None):\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2684141772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_geo_with_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"На Ленина пробки\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2684141772.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Извлечение геокодов и кластеризация...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2684141772.py\u001b[0m in \u001b[0;36mcluster_messages\u001b[0;34m(messages, model)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"geocode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_geocodes_enhanced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2684141772.py\u001b[0m in \u001b[0;36mextract_geocodes_enhanced\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# 2. Извлечение через NER (spacy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mner_geos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_geo_with_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgeo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mner_geos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2684141772.py\u001b[0m in \u001b[0;36mextract_geo_with_spacy\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ru_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ru_core_news_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    605\u001b[0m     )\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2244\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mdeserialize_vocab\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2220\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2221\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/vectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m def from_disk(\n\u001b[0m\u001b[1;32m   1440\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_cluster_pair.py\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Название модели\n",
        "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Параметры\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.5  # Порог косинусной близости\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def test_pair(text1: str, text2: str, model, threshold: float):\n",
        "    emb1 = model.encode([text1], convert_to_numpy=True)[0]\n",
        "    emb2 = model.encode([text2], convert_to_numpy=True)[0]\n",
        "\n",
        "    sim = cosine_similarity(emb1, emb2)\n",
        "    print(f\"Текст 1: {text1}\")\n",
        "    print(f\"Текст 2: {text2}\")\n",
        "    print(f\"Схожесть: {sim:.4f}\")\n",
        "    print(f\"Порог: {threshold}\")\n",
        "    print(f\"Результат: {'✅ Объединены' if sim >= threshold else '❌ Не объединены'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "def main():\n",
        "    print(\"Загрузка модели...\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')  # или 'cuda', если GPU доступен\n",
        "\n",
        "    # Примеры текстов для тестирования\n",
        "    pairs = [\n",
        "        (\n",
        "            \"На перекрестке Монтажников и Расточная три машины не поделили дорогу.\",\n",
        "            \"ДТП на Монтажников и Расточная. Дорогу не поделили, затор на час.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Свалка мусора на улице Кирова. Нужно убирать.\",\n",
        "            \"На Кирова снова свалка! Уже неделю никто не убирает.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Яма на Ленина, 15. Машина повреждена.\",\n",
        "            \"Пробка на Монтажников и Расточная.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В Екатеринбурге вынесли приговор банде бывших полицейских.\",\n",
        "            \"Суд в центре Екатеринбурга вынес приговор.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\",\n",
        "            \"Лицей на день закрыл младшие классы для санобработки помещений.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\",\n",
        "            \"В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\"\n",
        "        ),\n",
        "        (\n",
        "            \"Светофор не работает на перекрестке 8 Марта - Куйбышева. Водители просят быть внимательнее.\",\n",
        "            \"В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\"\n",
        "        ),\n",
        "        (\n",
        "            \"В гимназии № 35 открылся новый IT-технопарк для уроков робототехники и программирования.\",\n",
        "            \"В нашей гимназии открыйти айти-технопарк! Лютая имба!\"\n",
        "        ),\n",
        "        (\n",
        "            \"В небо над городом запустят праздничный салют в честь Дня города.\",\n",
        "            \"В городе прошел традиционный осенний легкоатлетический кросс.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(f\"Тестирование пар текстов с порогом {TEXT_SIMILARITY_THRESHOLD}...\\n\")\n",
        "\n",
        "    for text1, text2 in pairs:\n",
        "        test_pair(text1, text2, model, TEXT_SIMILARITY_THRESHOLD)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mimaYWaWUrRW",
        "outputId": "e6d7d01a-ea60-449b-90a5-4127ed56e7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка модели...\n",
            "Тестирование пар текстов с порогом 0.5...\n",
            "\n",
            "Текст 1: На перекрестке Монтажников и Расточная три машины не поделили дорогу.\n",
            "Текст 2: ДТП на Монтажников и Расточная. Дорогу не поделили, затор на час.\n",
            "Схожесть: 0.6976\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: Свалка мусора на улице Кирова. Нужно убирать.\n",
            "Текст 2: На Кирова снова свалка! Уже неделю никто не убирает.\n",
            "Схожесть: 0.4935\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: Яма на Ленина, 15. Машина повреждена.\n",
            "Текст 2: Пробка на Монтажников и Расточная.\n",
            "Схожесть: 0.4813\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В Екатеринбурге вынесли приговор банде бывших полицейских.\n",
            "Текст 2: Суд в центре Екатеринбурга вынес приговор.\n",
            "Схожесть: 0.6820\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\n",
            "Текст 2: Лицей на день закрыл младшие классы для санобработки помещений.\n",
            "Схожесть: 0.6516\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\n",
            "Текст 2: В лицее № 110 отменили занятия в начальных классах из-за проведения внеплановой дезинфекции.\n",
            "Схожесть: 0.6038\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: Светофор не работает на перекрестке 8 Марта - Куйбышева. Водители просят быть внимательнее.\n",
            "Текст 2: В детском саду № 222 на неделю закрывают группу из-за ремонта санузла.\n",
            "Схожесть: 0.0912\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В гимназии № 35 открылся новый IT-технопарк для уроков робототехники и программирования.\n",
            "Текст 2: В нашей гимназии открыйти айти-технопарк! Лютая имба!\n",
            "Схожесть: 0.4864\n",
            "Порог: 0.5\n",
            "Результат: ❌ Не объединены\n",
            "--------------------------------------------------\n",
            "Текст 1: В небо над городом запустят праздничный салют в честь Дня города.\n",
            "Текст 2: В городе прошел традиционный осенний легкоатлетический кросс.\n",
            "Схожесть: 0.5799\n",
            "Порог: 0.5\n",
            "Результат: ✅ Объединены\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Пытаемся улучшить"
      ],
      "metadata": {
        "id": "mhDK07UpsBVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_and_cluster_colab_spacy.py\n",
        "\n",
        "# Установка зависимостей (нужно запустить один раз)\n",
        "# pip install sentence-transformers scikit-learn rapidfuzz numpy spacy\n",
        "# python -m spacy download ru_core_news_sm\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import process\n",
        "import re\n",
        "\n",
        "# Пути к файлам\n",
        "MESSAGES_FILE = \"messages.json\"\n",
        "CLUSTERS_FILE = \"clusters_spacy.json\"\n",
        "\n",
        "# Название модели\n",
        "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Параметры\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.55\n",
        "EPS_DBSCAN = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "MIN_SAMPLES = 2\n",
        "GEOCODE_SIMILARITY_THRESHOLD = 0.5\n",
        "\n",
        "# Словарь топонимов Екатеринбурга\n",
        "TOPONYMS = [\n",
        "    \"Верх-Исетский\",\n",
        "    \"Железнодорожный\",\n",
        "    \"Кировский\",\n",
        "    \"Ленинский\",\n",
        "    \"Октябрьский\",\n",
        "    \"Орджоникидзевский\",\n",
        "    \"Чкаловский\",\n",
        "    \"Академический\",\n",
        "    \"Вторчермет\",\n",
        "    \"Втузгородок\",\n",
        "    \"Горный Щит\",\n",
        "    \"Елизаветинское\",\n",
        "    \"ЖБИ\",\n",
        "    \"Завокзальный\",\n",
        "    \"Изумрудный\",\n",
        "    \"Кольцово\",\n",
        "    \"Комсомольский\",\n",
        "    \"Короленковский\",\n",
        "    \"Малый Исток\",\n",
        "    \"Метеогорка\",\n",
        "    \"Нижнесвердловский\",\n",
        "    \"Новая Сортровка\",\n",
        "    \"Новобелореченский\",\n",
        "    \"Павелшина\",\n",
        "    \"Парковый\",\n",
        "    \"Пионерский\",\n",
        "    \"Психбольница\",\n",
        "    \"Рудничный\",\n",
        "    \"Семь Ключей\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Синие Камни\",\n",
        "    \"Сортировка\",\n",
        "    \"Старая Сортровка\",\n",
        "    \"Старый Октябрь\",\n",
        "    \"Татищева\",\n",
        "    \"Уктус\",\n",
        "    \"УНЦ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Центр\",\n",
        "    \"Черкасская\",\n",
        "    \"Шабровский\",\n",
        "    \"Шарташ\",\n",
        "    \"Широкая Речка\",\n",
        "    \"Эльмаш\",\n",
        "    \"Юго-Западный\",\n",
        "    \"8 Марта\",\n",
        "    \"Амундсена\",\n",
        "    \"Авиационная\",\n",
        "    \"Академика Павлова\",\n",
        "    \"Бабушкина\",\n",
        "    \"Бахчиванджи\",\n",
        "    \"Белинского\",\n",
        "    \"Бориса Ельцина\",\n",
        "    \"Братиславская\",\n",
        "    \"Бродова\",\n",
        "    \"Бульвар Академика Семихатова\",\n",
        "    \"Бутюлина\",\n",
        "    \"Валдайская\",\n",
        "    \"Верхняя Пышма\",\n",
        "    \"Вилонова\",\n",
        "    \"Водная\",\n",
        "    \"Волгоградская\",\n",
        "    \"Генеральская\",\n",
        "    \"Героев России\",\n",
        "    \"Глинки\",\n",
        "    \"Гоголя\",\n",
        "    \"Горького\",\n",
        "    \"Декабристов\",\n",
        "    \"Донбасская\",\n",
        "    \"Дружининская\",\n",
        "    \"Ереванская\",\n",
        "    \"Жуковского\",\n",
        "    \"Заводская\",\n",
        "    \"Зои Космодемьянской\",\n",
        "    \"Кирова\",\n",
        "    \"Колмогорова\",\n",
        "    \"Комсомольская\",\n",
        "    \"Короленко\",\n",
        "    \"Космонавтов\",\n",
        "    \"Крауля\",\n",
        "    \"Куйбышева\",\n",
        "    \"Лермонтова\",\n",
        "    \"Луначарского\",\n",
        "    \"Малышева\",\n",
        "    \"Мамина-Сибиряка\",\n",
        "    \"Маршала Жукова\",\n",
        "    \"Машинная\",\n",
        "    \"Мельникайте\",\n",
        "    \"Металлургов\",\n",
        "    \"Мира\",\n",
        "    \"Октябрьская\",\n",
        "    \"Павла Шпагина\",\n",
        "    \"Победы\",\n",
        "    \"Пролетарская\",\n",
        "    \"Проспект Космонавтов\",\n",
        "    \"Радищева\",\n",
        "    \"Репина\",\n",
        "    \"Розы Люксембург\",\n",
        "    \"Сакко и Ванцетти\",\n",
        "    \"Свердлова\",\n",
        "    \"Сибирский тракт\",\n",
        "    \"Софьи Ковалевской\",\n",
        "    \"Студенческая\",\n",
        "    \"Татищева\",\n",
        "    \"Техническая\",\n",
        "    \"Тургенева\",\n",
        "    \"Туполева\",\n",
        "    \"Учительская\",\n",
        "    \"Фрунзе\",\n",
        "    \"Чапаева\",\n",
        "    \"Чкалова\",\n",
        "    \"Шарташская\",\n",
        "    \"Шефская\",\n",
        "    \"Шиловская\",\n",
        "    \"Щорса\",\n",
        "    \"Энгельса\",\n",
        "    \"Площадь 1905 года\",\n",
        "    \"Площадь Труда\",\n",
        "    \"ЦУМ\",\n",
        "    \"Геологическая\",\n",
        "    \"ВИЗ\",\n",
        "    \"Уралмаш\",\n",
        "    \"Сакко\",\n",
        "    \"Геолка\",\n",
        "    \"1905\",\n",
        "    \"Уралмашь\",\n",
        "    \"Выксунский завод\",\n",
        "    \"Ленина\",\n",
        "    \"Екатеринбург\",\n",
        "    \"Расточная\",\n",
        "    \"Монтажников\",\n",
        "    \"УрФУ\",\n",
        "    \"НВК\",\n",
        "    \"Новокольцовский\",\n",
        "    \"Кирова\",\n",
        "    \"Лицей\",\n",
        "    \"Детский\",\n",
        "    \"Детсад\",\n",
        "    \"Школа\",\n",
        "    \"Больница\",\n",
        "    \"Университет\"\n",
        "]\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s\\dа-яё]', ' ', text.lower())\n",
        "\n",
        "def extract_geo_with_spacy(text: str) -> list[str]:\n",
        "    try:\n",
        "        import spacy\n",
        "\n",
        "        # Проверим, не пустой ли текст\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return []\n",
        "\n",
        "        nlp = spacy.load(\"ru_core_news_sm\")\n",
        "        doc = nlp(text)\n",
        "\n",
        "        geos = []\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in [\"LOC\", \"GPE\", \"FAC\"]:  # Location, Geopolitical Entity, Facility\n",
        "                geos.append(ent.text)\n",
        "        return geos\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при извлечении геокода через spacy: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_geocodes_enhanced(text: str) -> list[str]:\n",
        "    found = set()\n",
        "\n",
        "    # # 1. Fuzzy-поиск по словарю\n",
        "    # text_norm = normalize_text(text)\n",
        "    # for word in text_norm.split():\n",
        "    #     match, score, _ = process.extractOne(word, TOPONYMS)\n",
        "    #     if score >= 85:\n",
        "    #         found.add(match)\n",
        "\n",
        "    # 2. Извлечение через NER (spacy)\n",
        "    ner_geos = extract_geo_with_spacy(text)\n",
        "    for geo in ner_geos:\n",
        "        # Сопоставляем с каноническим словарём\n",
        "        match, score, _ = process.extractOne(geo, TOPONYMS)\n",
        "        if score >= 70:\n",
        "          found.add(geo)\n",
        "\n",
        "    # # 3. Регулярки для паттернов\n",
        "    # patterns = [\n",
        "    #     r'на\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"на Ленина\"\n",
        "    #     r'улица\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"улица Ленина\"\n",
        "    #     r'ул\\.\\s+([а-яё]+(?:\\s+[а-яё]+)?)',  # \"ул. Ленина\"\n",
        "    # ]\n",
        "    # for pattern in patterns:\n",
        "    #     matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "    #     for m in matches:\n",
        "    #         match, score, _ = process.extractOne(m, TOPONYMS)\n",
        "    #         if score >= 85:\n",
        "    #             found.add(match)\n",
        "\n",
        "    return list(found)\n",
        "\n",
        "def geocode_similarity(g1: list[str], g2: list[str], threshold=GEOCODE_SIMILARITY_THRESHOLD) -> bool:\n",
        "    if not g1 or not g2:\n",
        "        return False\n",
        "    set1 = set(g1)\n",
        "    set2 = set(g2)\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    if union == 0:\n",
        "        return False\n",
        "    return (intersection / union) >= threshold\n",
        "\n",
        "def cluster_messages(messages, model):\n",
        "    # Извлекаем геокоды\n",
        "    for msg in messages:\n",
        "        msg[\"geocode\"] = extract_geocodes_enhanced(msg[\"text\"])\n",
        "\n",
        "    # Разделяем на два потока\n",
        "    with_geo = [m for m in messages if m[\"geocode\"]]\n",
        "    without_geo = [m for m in messages if not m[\"geocode\"]]\n",
        "\n",
        "    clusters = []\n",
        "    cluster_id_counter = 1\n",
        "\n",
        "    # --- Часть 1: сообщения с геокодами ---\n",
        "    # Группируем по гео-похожести\n",
        "    geo_groups = []\n",
        "    for msg in with_geo:\n",
        "        assigned = False\n",
        "        for group in geo_groups:\n",
        "            if geocode_similarity(msg[\"geocode\"], group[0][\"geocode\"]):\n",
        "                group.append(msg)\n",
        "                assigned = True\n",
        "                break\n",
        "        if not assigned:\n",
        "            geo_groups.append([msg])\n",
        "\n",
        "    # Кластеризуем каждую гео-группу\n",
        "    for group in geo_groups:\n",
        "        if len(group) < MIN_SAMPLES:\n",
        "            continue\n",
        "\n",
        "        # Определяем, насколько геокоды однородны\n",
        "        all_geos = [g for m in group for g in m[\"geocode\"]]\n",
        "        unique_geos = set(all_geos)\n",
        "        # if len(unique_geos) == 1:\n",
        "        #     relaxed_threshold = max(0.3, TEXT_SIMILARITY_THRESHOLD - 0.1)\n",
        "        # elif len(unique_geos) <= 2:\n",
        "        #     relaxed_threshold = max(0.35, TEXT_SIMILARITY_THRESHOLD - 0.05)\n",
        "        # else:\n",
        "        relaxed_threshold = TEXT_SIMILARITY_THRESHOLD\n",
        "\n",
        "        eps = 1 - relaxed_threshold\n",
        "        texts = [m[\"text\"] for m in group]\n",
        "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "        clustering = DBSCAN(eps=eps, min_samples=MIN_SAMPLES, metric='cosine').fit(embeddings)\n",
        "\n",
        "        # Собираем кластеры\n",
        "        for label in set(clustering.labels_) - {-1}:\n",
        "            cluster_msgs = [group[i] for i, l in enumerate(clustering.labels_) if l == label]\n",
        "            if len(cluster_msgs) >= MIN_SAMPLES:\n",
        "                core_emb = np.mean(model.encode([m[\"text\"] for m in cluster_msgs], convert_to_numpy=True), axis=0)\n",
        "                clusters.append({\n",
        "                    \"cluster_id\": f\"cl_{cluster_id_counter:03d}\",\n",
        "                    \"geocode\": cluster_msgs[0][\"geocode\"],\n",
        "                    \"message_count\": len(cluster_msgs),\n",
        "                    \"first_seen\": min(m[\"date\"] for m in cluster_msgs),\n",
        "                    \"last_seen\": max(m[\"date\"] for m in cluster_msgs),\n",
        "                    \"core_embedding\": core_emb.tolist(),\n",
        "                    \"examples\": cluster_msgs\n",
        "                })\n",
        "                cluster_id_counter += 1\n",
        "\n",
        "    # --- Часть 2: сообщения без геокодов ---\n",
        "    if len(without_geo) >= MIN_SAMPLES:\n",
        "        texts = [m[\"text\"] for m in without_geo]\n",
        "        embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "        # Используем исходный порог (не меняем!)\n",
        "        eps = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "        clustering = DBSCAN(eps=eps, min_samples=MIN_SAMPLES, metric='cosine').fit(embeddings)\n",
        "\n",
        "        for label in set(clustering.labels_) - {-1}:\n",
        "            cluster_msgs = [without_geo[i] for i, l in enumerate(clustering.labels_) if l == label]\n",
        "            if len(cluster_msgs) >= MIN_SAMPLES:\n",
        "                core_emb = np.mean(model.encode([m[\"text\"] for m in cluster_msgs], convert_to_numpy=True), axis=0)\n",
        "                clusters.append({\n",
        "                    \"cluster_id\": f\"cl_{cluster_id_counter:03d}\",\n",
        "                    \"geocode\": [],  # явно указываем отсутствие\n",
        "                    \"message_count\": len(cluster_msgs),\n",
        "                    \"first_seen\": min(m[\"date\"] for m in cluster_msgs),\n",
        "                    \"last_seen\": max(m[\"date\"] for m in cluster_msgs),\n",
        "                    \"core_embedding\": core_emb.tolist(),\n",
        "                    \"examples\": cluster_msgs\n",
        "                })\n",
        "                cluster_id_counter += 1\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def main():\n",
        "    print(\"Загрузка модели L12...\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')\n",
        "\n",
        "    print(\"Загрузка сообщений...\")\n",
        "    with open(MESSAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        messages = json.load(f)\n",
        "\n",
        "    print(f\"Обнаружено {len(messages)} сообщений.\")\n",
        "\n",
        "    print(\"Извлечение геокодов и кластеризация...\")\n",
        "    clusters = cluster_messages(messages, model)\n",
        "\n",
        "    print(f\"Создано {len(clusters)} кластеров.\")\n",
        "\n",
        "    print(\"Сохранение кластеров...\")\n",
        "    with open(CLUSTERS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"clusters\": clusters}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Обработка завершена. Результат в {CLUSTERS_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(extract_geo_with_spacy(\"На Ленина пробки\"))\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCWH_9t3sEPv",
        "outputId": "8a8a2731-9390-41a8-ec0b-756ac2a76712"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "Загрузка модели L12...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка сообщений...\n",
            "Обнаружено 47 сообщений.\n",
            "Извлечение геокодов и кластеризация...\n",
            "Создано 13 кластеров.\n",
            "Сохранение кластеров...\n",
            "✅ Обработка завершена. Результат в clusters_spacy.json\n"
          ]
        }
      ]
    }
  ]
}