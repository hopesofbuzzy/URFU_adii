{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlf+mqf3UKU8cC0rz+dGPi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hopesofbuzzy/URFU_adii/blob/main/%D0%9E%D0%9F%D0%94/opd_project_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ipynb –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
        "–î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –≤—Å–µ –±–ª–æ–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É –≤:\n",
        "1) —Ç–æ–ø–æ–Ω–∏–º—ã –∏ –¥–æ–ø. —Å–µ–∫—Ü–∏–∏\n",
        "2) –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n",
        "\n",
        "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è - —ç—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π –ø—Ä—è–º–æ –Ω–∞ –º–µ—Å—Ç–µ"
      ],
      "metadata": {
        "id": "SHs2jxHb-fL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í–∞–∂–Ω–æ! –° VPS –º—ã –ø–æ–ª—É—á–∞–µ–º –∫–∞–∂–¥–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ª–∏—à—å –µ–¥–∏–Ω–æ–∂–¥—ã! –£—á–∏—Ç—ã–≤–∞–π—Ç–µ —ç—Ç–æ (—ç—Ç–æ —É—á—Ç–µ–Ω–æ –≤ –º–æ–¥—É–ª–µ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö)"
      ],
      "metadata": {
        "id": "jw76YoPLTjkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –¢–æ–ø–æ–Ω–∏–º—ã –∏ –¥–æ–ø. —Å–µ–∫—Ü–∏–∏"
      ],
      "metadata": {
        "id": "Dnv--shw-p53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzAsUQCJ8076"
      },
      "outputs": [],
      "source": [
        "# –°–ª–æ–≤–∞—Ä—å —Ç–æ–ø–æ–Ω–∏–º–æ–≤ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞\n",
        "TOPONYMS = [\n",
        "    \"–í–µ—Ä—Ö-–ò—Å–µ—Ç—Å–∫–∏–π\",\n",
        "    \"–ñ–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–π\",\n",
        "    \"–ö–∏—Ä–æ–≤—Å–∫–∏–π\",\n",
        "    \"–õ–µ–Ω–∏–Ω—Å–∫–∏–π\",\n",
        "    \"–û–∫—Ç—è–±—Ä—å—Å–∫–∏–π\",\n",
        "    \"–û—Ä–¥–∂–æ–Ω–∏–∫–∏–¥–∑–µ–≤—Å–∫–∏–π\",\n",
        "    \"–ß–∫–∞–ª–æ–≤—Å–∫–∏–π\",\n",
        "    \"–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π\",\n",
        "    \"–í—Ç–æ—Ä—á–µ—Ä–º–µ—Ç\",\n",
        "    \"–í—Ç—É–∑–≥–æ—Ä–æ–¥–æ–∫\",\n",
        "    \"–ì–æ—Ä–Ω—ã–π –©–∏—Ç\",\n",
        "    \"–ï–ª–∏–∑–∞–≤–µ—Ç–∏–Ω—Å–∫–æ–µ\",\n",
        "    \"–ñ–ë–ò\",\n",
        "    \"–ó–∞–≤–æ–∫–∑–∞–ª—å–Ω—ã–π\",\n",
        "    \"–ò–∑—É–º—Ä—É–¥–Ω—ã–π\",\n",
        "    \"–ö–æ–ª—å—Ü–æ–≤–æ\",\n",
        "    \"–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∏–π\",\n",
        "    \"–ö–æ—Ä–æ–ª–µ–Ω–∫–æ–≤—Å–∫–∏–π\",\n",
        "    \"–ú–∞–ª—ã–π –ò—Å—Ç–æ–∫\",\n",
        "    \"–ú–µ—Ç–µ–æ–≥–æ—Ä–∫–∞\",\n",
        "    \"–ù–∏–∂–Ω–µ—Å–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∏–π\",\n",
        "    \"–ù–æ–≤–∞—è –°–æ—Ä—Ç—Ä–æ–≤–∫–∞\",\n",
        "    \"–ù–æ–≤–æ–±–µ–ª–æ—Ä–µ—á–µ–Ω—Å–∫–∏–π\",\n",
        "    \"–ü–∞–≤–µ–ª—à–∏–Ω–∞\",\n",
        "    \"–ü–∞—Ä–∫–æ–≤—ã–π\",\n",
        "    \"–ü–∏–æ–Ω–µ—Ä—Å–∫–∏–π\",\n",
        "    \"–ü—Å–∏—Ö–±–æ–ª—å–Ω–∏—Ü–∞\",\n",
        "    \"–†—É–¥–Ω–∏—á–Ω—ã–π\",\n",
        "    \"–°–µ–º—å –ö–ª—é—á–µ–π\",\n",
        "    \"–°–∏–±–∏—Ä—Å–∫–∏–π —Ç—Ä–∞–∫—Ç\",\n",
        "    \"–°–∏–Ω–∏–µ –ö–∞–º–Ω–∏\",\n",
        "    \"–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞\",\n",
        "    \"–°—Ç–∞—Ä–∞—è –°–æ—Ä—Ç—Ä–æ–≤–∫–∞\",\n",
        "    \"–°—Ç–∞—Ä—ã–π –û–∫—Ç—è–±—Ä—å\",\n",
        "    \"–¢–∞—Ç–∏—â–µ–≤–∞\",\n",
        "    \"–£–∫—Ç—É—Å\",\n",
        "    \"–£–ù–¶\",\n",
        "    \"–£—Ä–∞–ª–º–∞—à\",\n",
        "    \"–¶–µ–Ω—Ç—Ä\",\n",
        "    \"–ß–µ—Ä–∫–∞—Å—Å–∫–∞—è\",\n",
        "    \"–®–∞–±—Ä–æ–≤—Å–∫–∏–π\",\n",
        "    \"–®–∞—Ä—Ç–∞—à\",\n",
        "    \"–®–∏—Ä–æ–∫–∞—è –†–µ—á–∫–∞\",\n",
        "    \"–≠–ª—å–º–∞—à\",\n",
        "    \"–Æ–≥–æ-–ó–∞–ø–∞–¥–Ω—ã–π\",\n",
        "    \"8 –ú–∞—Ä—Ç–∞\",\n",
        "    \"–ê–º—É–Ω–¥—Å–µ–Ω–∞\",\n",
        "    \"–ê–≤–∏–∞—Ü–∏–æ–Ω–Ω–∞—è\",\n",
        "    \"–ê–∫–∞–¥–µ–º–∏–∫–∞ –ü–∞–≤–ª–æ–≤–∞\",\n",
        "    \"–ë–∞–±—É—à–∫–∏–Ω–∞\",\n",
        "    \"–ë–∞—Ö—á–∏–≤–∞–Ω–¥–∂–∏\",\n",
        "    \"–ë–µ–ª–∏–Ω—Å–∫–æ–≥–æ\",\n",
        "    \"–ë–æ—Ä–∏—Å–∞ –ï–ª—å—Ü–∏–Ω–∞\",\n",
        "    \"–ë—Ä–∞—Ç–∏—Å–ª–∞–≤—Å–∫–∞—è\",\n",
        "    \"–ë—Ä–æ–¥–æ–≤–∞\",\n",
        "    \"–ë—É–ª—å–≤–∞—Ä –ê–∫–∞–¥–µ–º–∏–∫–∞ –°–µ–º–∏—Ö–∞—Ç–æ–≤–∞\",\n",
        "    \"–ë—É—Ç—é–ª–∏–Ω–∞\",\n",
        "    \"–í–∞–ª–¥–∞–π—Å–∫–∞—è\",\n",
        "    \"–í–µ—Ä—Ö–Ω—è—è –ü—ã—à–º–∞\",\n",
        "    \"–í–∏–ª–æ–Ω–æ–≤–∞\",\n",
        "    \"–í–æ–¥–Ω–∞—è\",\n",
        "    \"–í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–∞—è\",\n",
        "    \"–ì–µ–Ω–µ—Ä–∞–ª—å—Å–∫–∞—è\",\n",
        "    \"–ì–µ—Ä–æ–µ–≤ –†–æ—Å—Å–∏–∏\",\n",
        "    \"–ì–ª–∏–Ω–∫–∏\",\n",
        "    \"–ì–æ–≥–æ–ª—è\",\n",
        "    \"–ì–æ—Ä—å–∫–æ–≥–æ\",\n",
        "    \"–î–µ–∫–∞–±—Ä–∏—Å—Ç–æ–≤\",\n",
        "    \"–î–æ–Ω–±–∞—Å—Å–∫–∞—è\",\n",
        "    \"–î—Ä—É–∂–∏–Ω–∏–Ω—Å–∫–∞—è\",\n",
        "    \"–ï—Ä–µ–≤–∞–Ω—Å–∫–∞—è\",\n",
        "    \"–ñ—É–∫–æ–≤—Å–∫–æ–≥–æ\",\n",
        "    \"–ó–∞–≤–æ–¥—Å–∫–∞—è\",\n",
        "    \"–ó–æ–∏ –ö–æ—Å–º–æ–¥–µ–º—å—è–Ω—Å–∫–æ–π\",\n",
        "    \"–ö–∏—Ä–æ–≤–∞\",\n",
        "    \"–ö–æ–ª–º–æ–≥–æ—Ä–æ–≤–∞\",\n",
        "    \"–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è\",\n",
        "    \"–ö–æ—Ä–æ–ª–µ–Ω–∫–æ\",\n",
        "    \"–ö–æ—Å–º–æ–Ω–∞–≤—Ç–æ–≤\",\n",
        "    \"–ö—Ä–∞—É–ª—è\",\n",
        "    \"–ö—É–π–±—ã—à–µ–≤–∞\",\n",
        "    \"–õ–µ—Ä–º–æ–Ω—Ç–æ–≤–∞\",\n",
        "    \"–õ—É–Ω–∞—á–∞—Ä—Å–∫–æ–≥–æ\",\n",
        "    \"–ú–∞–ª—ã—à–µ–≤–∞\",\n",
        "    \"–ú–∞–º–∏–Ω–∞-–°–∏–±–∏—Ä—è–∫–∞\",\n",
        "    \"–ú–∞—Ä—à–∞–ª–∞ –ñ—É–∫–æ–≤–∞\",\n",
        "    \"–ú–∞—à–∏–Ω–Ω–∞—è\",\n",
        "    \"–ú–µ–ª—å–Ω–∏–∫–∞–π—Ç–µ\",\n",
        "    \"–ú–µ—Ç–∞–ª–ª—É—Ä–≥–æ–≤\",\n",
        "    \"–ú–∏—Ä–∞\",\n",
        "    \"–û–∫—Ç—è–±—Ä—å—Å–∫–∞—è\",\n",
        "    \"–ü–∞–≤–ª–∞ –®–ø–∞–≥–∏–Ω–∞\",\n",
        "    \"–ü–æ–±–µ–¥—ã\",\n",
        "    \"–ü—Ä–æ–ª–µ—Ç–∞—Ä—Å–∫–∞—è\",\n",
        "    \"–ü—Ä–æ—Å–ø–µ–∫—Ç –ö–æ—Å–º–æ–Ω–∞–≤—Ç–æ–≤\",\n",
        "    \"–†–∞–¥–∏—â–µ–≤–∞\",\n",
        "    \"–†–µ–ø–∏–Ω–∞\",\n",
        "    \"–†–æ–∑—ã –õ—é–∫—Å–µ–º–±—É—Ä–≥\",\n",
        "    \"–°–∞–∫–∫–æ –∏ –í–∞–Ω—Ü–µ—Ç—Ç–∏\",\n",
        "    \"–°–≤–µ—Ä–¥–ª–æ–≤–∞\",\n",
        "    \"–°–∏–±–∏—Ä—Å–∫–∏–π —Ç—Ä–∞–∫—Ç\",\n",
        "    \"–°–æ—Ñ—å–∏ –ö–æ–≤–∞–ª–µ–≤—Å–∫–æ–π\",\n",
        "    \"–°—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è\",\n",
        "    \"–¢–∞—Ç–∏—â–µ–≤–∞\",\n",
        "    \"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è\",\n",
        "    \"–¢—É—Ä–≥–µ–Ω–µ–≤–∞\",\n",
        "    \"–¢—É–ø–æ–ª–µ–≤–∞\",\n",
        "    \"–£—á–∏—Ç–µ–ª—å—Å–∫–∞—è\",\n",
        "    \"–§—Ä—É–Ω–∑–µ\",\n",
        "    \"–ß–∞–ø–∞–µ–≤–∞\",\n",
        "    \"–ß–∫–∞–ª–æ–≤–∞\",\n",
        "    \"–®–∞—Ä—Ç–∞—à—Å–∫–∞—è\",\n",
        "    \"–®–µ—Ñ—Å–∫–∞—è\",\n",
        "    \"–®–∏–ª–æ–≤—Å–∫–∞—è\",\n",
        "    \"–©–æ—Ä—Å–∞\",\n",
        "    \"–≠–Ω–≥–µ–ª—å—Å–∞\",\n",
        "    \"–ü–ª–æ—â–∞–¥—å 1905 –≥–æ–¥–∞\",\n",
        "    \"–ü–ª–æ—â–∞–¥—å –¢—Ä—É–¥–∞\",\n",
        "    \"–¶–£–ú\",\n",
        "    \"–ì–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è\",\n",
        "    \"–í–ò–ó\",\n",
        "    \"–£—Ä–∞–ª–º–∞—à\",\n",
        "    \"–°–∞–∫–∫–æ\",\n",
        "    \"–ì–µ–æ–ª–∫–∞\",\n",
        "    \"1905\",\n",
        "    \"–£—Ä–∞–ª–º–∞—à—å\",\n",
        "    \"–í—ã–∫—Å—É–Ω—Å–∫–∏–π –∑–∞–≤–æ–¥\",\n",
        "    \"–õ–µ–Ω–∏–Ω–∞\",\n",
        "    \"–†–∞—Å—Ç–æ—á–Ω–∞—è\",\n",
        "    \"–ú–æ–Ω—Ç–∞–∂–Ω–∏–∫–æ–≤\",\n",
        "    \"–£—Ä–§–£\",\n",
        "    \"–ù–í–ö\",\n",
        "    \"–ù–æ–≤–æ–∫–æ–ª—å—Ü–æ–≤—Å–∫–∏–π\",\n",
        "    \"–ö–∏—Ä–æ–≤–∞\",\n",
        "    \"–õ–∏—Ü–µ–π\",\n",
        "    \"–î–µ—Ç—Å–∫–∏–π\",\n",
        "    \"–î–µ—Ç—Å–∞–¥\",\n",
        "    \"–®–∫–æ–ª–∞\",\n",
        "    \"–ë–æ–ª—å–Ω–∏—Ü–∞\",\n",
        "    \"–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"
      ],
      "metadata": {
        "id": "fuT6hoWlG63S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
        "INPUT_FILE = \"messages.json\"\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    print(f\"‚ùå –§–∞–π–ª {INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –µ–≥–æ –≤ Colab.\")\n",
        "    exit()\n",
        "\n",
        "# –ß—Ç–µ–Ω–∏–µ\n",
        "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    messages = json.load(f)\n",
        "\n",
        "print(f\"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π.\")\n",
        "\n",
        "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ ID\n",
        "updated = 0\n",
        "for msg in messages:\n",
        "    if \"id\" not in msg or not msg[\"id\"]:\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º URL –∫–∞–∫ –æ—Å–Ω–æ–≤—É (–ª—É—á—à–∏–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è Telegram)\n",
        "        url = msg.get(\"url\", \"\").strip()\n",
        "        if url:\n",
        "            # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å–ª–µ '?' (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)\n",
        "            clean_url = url.split('?')[0].strip()\n",
        "            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ö—ç—à (16 —Å–∏–º–≤–æ–ª–æ–≤, –∫–∞–∫ –≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ)\n",
        "            msg_id = hashlib.sha256(clean_url.encode('utf-8')).hexdigest()[:16]\n",
        "        else:\n",
        "            # Fallback: —Ö—ç—à –æ—Ç —Ç–µ–∫—Å—Ç–∞ + –¥–∞—Ç—ã\n",
        "            fallback = msg.get(\"text\", \"\") + msg.get(\"date\", \"\")\n",
        "            msg_id = hashlib.md5(fallback.encode('utf-8')).hexdigest()[:16]\n",
        "\n",
        "        msg[\"id\"] = msg_id\n",
        "        updated += 1\n",
        "\n",
        "print(f\"üÜï –î–æ–±–∞–≤–ª–µ–Ω–æ ID –¥–ª—è {updated} —Å–æ–æ–±—â–µ–Ω–∏–π.\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "with open(INPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(messages, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"‚úÖ –§–∞–π–ª {INPUT_FILE} –æ–±–Ω–æ–≤–ª—ë–Ω.\")"
      ],
      "metadata": {
        "id": "4lJB2MZGZ_0l",
        "outputId": "843da751-3856-4743-991d-434c00e1fa4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ 72 —Å–æ–æ–±—â–µ–Ω–∏–π.\n",
            "üÜï –î–æ–±–∞–≤–ª–µ–Ω–æ ID –¥–ª—è 72 —Å–æ–æ–±—â–µ–Ω–∏–π.\n",
            "‚úÖ –§–∞–π–ª messages.json –æ–±–Ω–æ–≤–ª—ë–Ω.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gigachat"
      ],
      "metadata": {
        "id": "Y2HK4TyhCyBm",
        "outputId": "3fef25f2-dc91-431a-e233-c4d4748afbae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gigachat in /usr/local/lib/python3.12/dist-packages (0.1.43)\n",
            "Requirement already satisfied: httpx<1 in /usr/local/lib/python3.12/dist-packages (from gigachat) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=1 in /usr/local/lib/python3.12/dist-packages (from gigachat) (2.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1->gigachat) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1->gigachat) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1->gigachat) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from gigachat import GigaChat\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "CATEGORIES = [\n",
        "    \"–±–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\",\n",
        "    \"–ñ–ö–•\",\n",
        "    \"—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç\",\n",
        "    \"—Ç—É—Ä–∏–∑–º\",\n",
        "    \"–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\",\n",
        "    \"–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\",\n",
        "    \"—Å–ø–æ—Ä—Ç\"\n",
        "]\n",
        "\n",
        "AUTH = userdata.get('SBER_AUTH')\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def classify_and_analyze(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    –û–¥–∏–Ω –≤—ã–∑–æ–≤ GigaChat –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç:\n",
        "    - –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–∏–∑ 7 –∑–∞–¥–∞–Ω–Ω—ã—Ö),\n",
        "    - —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å,\n",
        "    - —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ, —Ç–µ–∫—É—â–µ–≥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞.\n",
        "\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {\"category\": str, \"sentiment\": str, \"is_critical\": bool}\n",
        "    \"\"\"\n",
        "    if not AUTH:\n",
        "        raise ValueError(\"GIGACHAT_TOKEN –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–¥–∞–Ω –≤ .env\")\n",
        "\n",
        "    categories_str = \", \".join(CATEGORIES)\n",
        "    prompt = f\"\"\"\n",
        "        –¢—ã ‚Äî AI-–ø–æ–º–æ—â–Ω–∏–∫ –ì–ª–∞–≤—ã –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.\n",
        "        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –æ—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π:\n",
        "\n",
        "        {{\n",
        "          \"category\": \"–æ–¥–Ω–∞ –∏–∑: {categories_str}\",\n",
        "          \"sentiment\": \"–Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–π—Ç—Ä –∏–ª–∏ –ø–æ–∑–∏—Ç–∏–≤\",\n",
        "          \"is_critical_incident\": true\n",
        "        }}\n",
        "\n",
        "        –ü—Ä–∞–≤–∏–ª–∞:\n",
        "        - \"is_critical_incident\" = true, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç **—Ä–µ–∞–ª—å–Ω—ã–π –∏–Ω—Ü–∏–¥–µ–Ω—Ç, –ø—Ä–æ–∏–∑–æ—à–µ–¥—à–∏–π –Ω–µ–¥–∞–≤–Ω–æ –∏–ª–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–∏–π —Å–µ–π—á–∞—Å** (–Ω–∞–ø—Ä–∏–º–µ—Ä: –∞–≤–∞—Ä–∏—è, –ø—Ä–æ—Ä—ã–≤ —Ç—Ä—É–±—ã, –î–¢–ü, –Ω–µ–∑–∞–∫–æ–Ω–Ω–∞—è —Å–≤–∞–ª–∫–∞, –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —Å–≤–µ—Ç–∞ –∏ —Ç.–¥.).\n",
        "        - \"is_critical_incident\" = false, –µ—Å–ª–∏ —ç—Ç–æ: —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ–≥–æ, –≥–∏–ø–æ—Ç–µ–∑–∞, —à—É—Ç–∫–∞, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ, —Å—Ç—Ä–∞—Ö –∏–ª–∏ –æ–±—â–∞—è —Ñ—Ä–∞–∑–∞ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏.\n",
        "\n",
        "        –¢–µ–∫—Å—Ç:\n",
        "        \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with GigaChat(credentials=AUTH, verify_ssl_certs=False) as giga:\n",
        "            response = giga.chat(prompt)\n",
        "\n",
        "        raw = response.choices[0].message.content.strip()\n",
        "\n",
        "        # –£–±–∏—Ä–∞–µ–º markdown-–æ–±—ë—Ä—Ç–∫—É (```json ... ```)\n",
        "        if raw.startswith(\"```\"):\n",
        "            raw = \"\\n\".join(raw.split(\"\\n\")[1:-1])\n",
        "\n",
        "        data = json.loads(raw)\n",
        "\n",
        "        category = data.get(\"category\", \"–¥—Ä—É–≥–æ–µ\")\n",
        "        sentiment = data.get(\"sentiment\", \"–Ω–µ–π—Ç—Ä\")\n",
        "        is_critical = bool(data.get(\"is_critical_incident\", False))\n",
        "\n",
        "        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "        if category not in CATEGORIES:\n",
        "            category = \"–¥—Ä—É–≥–æ–µ\"\n",
        "\n",
        "        return {\n",
        "            \"category\": category,\n",
        "            \"sentiment\": sentiment,\n",
        "            \"is_critical\": is_critical\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ GigaChat: {e}\")\n",
        "        return {\n",
        "            \"category\": \"–¥—Ä—É–≥–æ–µ\",\n",
        "            \"sentiment\": \"–Ω–µ–π—Ç—Ä\",\n",
        "            \"is_critical\": False\n",
        "        }"
      ],
      "metadata": {
        "id": "bABzkupwB-cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"messages.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    messages = json.load(f)\n",
        "\n",
        "for msg in messages:\n",
        "    result = classify_and_analyze(msg[\"text\"])\n",
        "    msg[\"category\"] = result[\"category\"]\n",
        "    msg[\"sentiment\"] = result[\"sentiment\"]\n",
        "    msg[\"is_critical\"] = result[\"is_critical\"]\n",
        "\n",
        "with open(\"messages_from_vps.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(messages, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "eXNNH0ChCNWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from classification.topic_classifier import classify_and_analyze\n",
        "\n",
        "with open(MESSAGES, \"r\", encoding=\"utf-8\") as f:\n",
        "¬† ¬† messages = json.load(f)\n",
        "\n",
        "for msg in messages:\n",
        "¬† ¬† result = classify_and_analyze(msg[\"text\"])\n",
        "¬† ¬† msg[\"category\"] = result[\"category\"]\n",
        "¬† ¬† msg[\"sentiment\"] = result[\"sentiment\"]\n",
        "¬† ¬† msg[\"is_critical\"] = result[\"is_critical\"]\n",
        "\n",
        "with open(CLASSIFIED_MESSAGES, \"w\", encoding=\"utf-8\") as f:\n",
        "¬† ¬† json.dump(messages, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "fsZrIa5w9NOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"
      ],
      "metadata": {
        "id": "FikyQQwJ-tPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ú—ã —á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å VPS, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ VPS"
      ],
      "metadata": {
        "id": "97sqc5ifws9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–¥–∏–Ω —Ä–∞–∑)\n",
        "!pip install -q sentence-transformers scikit-learn rapidfuzz spacy geopy gigachat\n",
        "\n",
        "# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò SPACY (–æ–¥–∏–Ω —Ä–∞–∑)\n",
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"ru_core_news_sm\"])\n",
        "\n",
        "# –ò–ú–ü–û–†–¢–´\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from rapidfuzz import process\n",
        "import spacy\n",
        "\n",
        "# GigaChat\n",
        "from gigachat import GigaChat\n",
        "from gigachat.models import Chat, Messages, MessagesRole\n",
        "from google.colab import userdata\n",
        "\n",
        "# Geocoding\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.extra.rate_limiter import RateLimiter\n",
        "\n",
        "# ======================\n",
        "# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø\n",
        "# ======================\n",
        "\n",
        "# –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¢–û–õ–¨–ö–û –∫–∞–∫ –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è VPS\n",
        "MESSAGES_FILE = \"messages_from_vps.json\"      # –≤—Ö–æ–¥ (–æ—Ç VPS)\n",
        "CLUSTERS_PROCESSED_FILE = \"clusters_to_vps.json\"  # –≤—ã—Ö–æ–¥ (–Ω–∞ VPS)\n",
        "CACHE_DIR = \"cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "GEO_CACHE_FILE = os.path.join(CACHE_DIR, \"geocodes_cache.json\")\n",
        "PROCESSED_IDS_FILE = os.path.join(CACHE_DIR, \"processed_ids.json\")\n",
        "NOISE_BUFFER_FILE = os.path.join(CACHE_DIR, \"noise_buffer.json\")\n",
        "\n",
        "# –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
        "MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "TEXT_SIMILARITY_THRESHOLD = 0.45\n",
        "EPS_DBSCAN = 1 - TEXT_SIMILARITY_THRESHOLD\n",
        "MIN_SAMPLES = 2\n",
        "\n",
        "# TTL –¥–ª—è –±—É—Ñ–µ—Ä–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (—á–∞—Å—ã)\n",
        "NOISE_TTL_HOURS = 24\n",
        "# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "MAX_NOISE_BUFFER_SIZE = 200\n",
        "\n",
        "# –ì–µ–æ–∫–æ–¥–µ—Ä (OpenStreetMap)\n",
        "geolocator = Nominatim(user_agent=\"ekb_municipal_ai_hackathon\")\n",
        "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
        "\n",
        "# GigaChat\n",
        "AUTH = userdata.get('SBER_AUTH')\n",
        "GIGA_MODEL = \"GigaChat-2\"\n",
        "\n",
        "# ======================\n",
        "# VPS –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø (–ó–ê–ì–õ–£–®–ö–ò)\n",
        "# ======================\n",
        "\n",
        "VPS_API_URL = \"https://your-vps-domain.com/api\"  # ‚Üê –ó–ê–ú–ï–ù–ò –ù–ê –†–ï–ê–õ–¨–ù–´–ô URL\n",
        "VPS_API_KEY = \"your-secret-api-key\"             # ‚Üê –õ—É—á—à–µ –∏–∑ userdata\n",
        "\n",
        "def fetch_messages_from_vps():\n",
        "    print(\"üì• –ó–∞–ø—Ä–æ—Å –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å VPS...\")\n",
        "    # –ó–ê–ì–õ–£–®–ö–ê: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª\n",
        "    if not os.path.exists(MESSAGES_FILE):\n",
        "        return []\n",
        "    with open(MESSAGES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# –ù–µ –∑–Ω–∞—é, –Ω—É–∂–Ω–æ –ª–∏ —Å—á–∏—Ç—ã–≤–∞—Ç—å, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –Ω–µ—Ç, —Ç–∞–∫ —á—Ç–æ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ —ç—Ç–æ\n",
        "# –°—á–∏—Ç—ã–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ\n",
        "def fetch_clusters_from_vps():\n",
        "    # –ú–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –¥–ª—è MVP, —Ç–∞–∫ –∫–∞–∫ –∫–ª–∞—Å—Ç–µ—Ä—ã –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –º—ã\n",
        "    print(\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—É—â–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å VPS...\")\n",
        "    # –ó–ê–ì–õ–£–®–ö–ê: —á–∏—Ç–∞–µ–º –∏–∑ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞\n",
        "    if os.path.exists(CLUSTERS_PROCESSED_FILE):\n",
        "        with open(CLUSTERS_PROCESSED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f).get(\"clusters\", [])\n",
        "    return []\n",
        "\n",
        "def send_clusters_to_vps(clusters_data):\n",
        "    print(\"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ VPS...\")\n",
        "    # –ó–ê–ì–õ–£–®–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ\n",
        "    with open(CLUSTERS_PROCESSED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(clusters_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ======================\n",
        "# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò\n",
        "# ======================\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s\\d–∞-—è—ë]', ' ', text.lower())\n",
        "\n",
        "def extract_geo_with_spacy(text: str) -> list[str]:\n",
        "    try:\n",
        "        nlp = spacy.load(\"ru_core_news_sm\")\n",
        "        doc = nlp(text)\n",
        "        return [ent.text for ent in doc.ents if ent.label_ in (\"LOC\", \"GPE\", \"FAC\")]\n",
        "    except Exception as e:\n",
        "        print(f\"[spaCy] –û—à–∏–±–∫–∞: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_geo_with_gigachat(text: str) -> list[str]:\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∂–∞–ª–æ–± –∂–∏—Ç–µ–ª–µ–π –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞. \"\n",
        "        \"–ò–∑–≤–ª–µ–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ç–æ–ª—å–∫–æ –º–∞–ª—ã–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã (—Ç–æ–ª—å–∫–æ —É–ª–∏—Ü—ã, —Ä–∞–π–æ–Ω—ã, –∑–¥–∞–Ω–∏—è, –ø–ª–æ—â–∞–¥–∏ –∏ –∏–Ω—ã–µ –º–∞–ª–µ–Ω—å–∫–∏–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏) \"\n",
        "        \"–∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤ –≤ –ø–æ–ª–µ 'geocode'.\"\n",
        "    )\n",
        "    USER_PROMPT = f\"\"\"\n",
        "      –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è: \"{text[:150]}\"\n",
        "      –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–≤–æ–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.\n",
        "      –í—Ö–æ–¥: \"–í—á–µ—Ä–∞ –≤ –±–∞—Å—Å–µ–π–Ω–µ '–Æ–Ω–æ—Å—Ç—å' –ø—Ä–æ–∏–∑–æ—à–ª–∞\"\n",
        "      –í—ã—Ö–æ–¥: {{\"geocode\": [\"–Æ–Ω–æ—Å—Ç—å\", \"–±–∞—Å—Å–µ–π–Ω\"]}}\n",
        "      –í—Ö–æ–¥: \"–ù–∞ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–µ –ú–æ–Ω—Ç–∞–∂–Ω–∏–∫–æ–≤ - –†–∞—Å—Ç–æ—á–Ω–∞—è –∞–≤–∞—Ä–∏—è\",\n",
        "      –í—ã—Ö–æ–¥: {{\"geocode\": [\"–ú–æ–Ω—Ç–∞–∂–Ω–∏–∫–æ–≤\", \"–†–∞—Å—Ç–æ—á–Ω–∞—è\"]}}\n",
        "      –í—Ö–æ–¥: \"–†–µ–º–æ–Ω—Ç –Ω–∞ —É–ª–∏—Ü–µ –ú–æ–Ω—Ç–∞–∂–Ω–∏–∫–æ–≤ –∑–∞—Ç—è–Ω—É–ª—Å—è.\"\n",
        "      –í—ã—Ö–æ–¥: {{\"geocode\": [\"–ú–æ–Ω—Ç–∞–∂–Ω–∏–∫–æ–≤\"]}}\n",
        "      –í—Ö–æ–¥: \"–î–µ—Ç–∏ –∏–∑ –°–û–® ‚Ññ3 –∂–∞–ª—É—é—Ç—Å—è –Ω–∞ –æ—Ç–æ–ø–ª–µ–Ω–∏–µ.\"\n",
        "      –í—ã—Ö–æ–¥: {{\"geocode\": [\"—à–∫–æ–ª–∞ ‚Ññ3\"]}}\n",
        "      –í—Ö–æ–¥: \"–í—á–µ—Ä–∞ –≤ '–Æ–Ω–æ—Å—Ç–∏' –æ–±–µ–¥–∞–ª–∏\"\n",
        "      –í—ã—Ö–æ–¥: {{\"geocode\": [\"–Æ–Ω–æ—Å—Ç—å\", \"–æ–±–µ–¥\"]}}\n",
        "      –í–∞–∂–Ω–æ: –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–π –ª–∏—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏! –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–π –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤, —Å—Ç—Ä–∞–Ω, –æ–±–ª–∞—Å—Ç–µ–π!\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        Messages(role=MessagesRole.SYSTEM, content=SYSTEM_PROMPT),\n",
        "        Messages(role=MessagesRole.USER, content=USER_PROMPT),\n",
        "    ]\n",
        "\n",
        "    payload = Chat(messages=messages, temperature=0.1, max_tokens=200)\n",
        "\n",
        "    try:\n",
        "        with GigaChat(credentials=AUTH, model=GIGA_MODEL, verify_ssl_certs=False) as giga:\n",
        "            resp = giga.chat(payload)\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "            if content.startswith(\"```\"):\n",
        "                content = content.split(\"```\")[1] if \"```\" in content else content\n",
        "            if content.startswith(\"json\"):\n",
        "                content = content[4:].strip()\n",
        "            data = json.loads(content)\n",
        "            return data.get(\"geocode\", [])\n",
        "    except Exception as e:\n",
        "        print(f\"[GigaChat] –û—à–∏–±–∫–∞: {e}\")\n",
        "        return []\n",
        "\n",
        "def geocode_toponyms(toponyms: list[str], city=\"–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥\") -> list[dict]:\n",
        "    results = []\n",
        "    for topo in toponyms:\n",
        "        query = f\"{topo}, {city}\"\n",
        "        try:\n",
        "            loc = geocode(query, country_codes=\"RU\", timeout=10)\n",
        "            if loc:\n",
        "                results.append({\n",
        "                    \"name\": topo,\n",
        "                    \"lat\": round(loc.latitude, 6),\n",
        "                    \"lon\": round(loc.longitude, 6)\n",
        "                })\n",
        "            else:\n",
        "                results.append({\"name\": topo, \"lat\": None, \"lon\": None})\n",
        "        except Exception as e:\n",
        "            print(f\"[Nominatim] {e}\")\n",
        "            results.append({\"name\": topo, \"lat\": None, \"lon\": None})\n",
        "    return results\n",
        "\n",
        "def extract_geocodes_cached(msg_id: str, text: str, geocache: dict) -> dict:\n",
        "    if msg_id in geocache:\n",
        "        return geocache[msg_id]\n",
        "\n",
        "    found = set()\n",
        "    ner_geos = extract_geo_with_spacy(text)\n",
        "    for geo in ner_geos:\n",
        "        match, score, _ = process.extractOne(geo, TOPONYMS)\n",
        "        if score >= 85:\n",
        "            found.add(match)\n",
        "\n",
        "    source = \"spacy\"\n",
        "    if not found:\n",
        "        found = set(extract_geo_with_gigachat(text))\n",
        "        source = \"gigachat\"\n",
        "\n",
        "    found = list(found)\n",
        "    coords = geocode_toponyms(found) if found else []\n",
        "\n",
        "    result = {\n",
        "        \"geocode\": found,\n",
        "        \"coords\": coords,\n",
        "        \"source\": source,\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "    geocache[msg_id] = result\n",
        "    return result\n",
        "\n",
        "def load_cache():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –≥–µ–æ–∫–æ–¥–æ–≤ –∏ –±—É—Ñ–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö ID —Å–æ–æ–±—â–µ–Ω–∏–π\"\"\"\n",
        "    geocache = {}\n",
        "    processed_ids = set()\n",
        "    if os.path.exists(GEO_CACHE_FILE):\n",
        "        with open(GEO_CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            geocache = json.load(f)\n",
        "    if os.path.exists(PROCESSED_IDS_FILE):\n",
        "        with open(PROCESSED_IDS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            processed_ids = set(json.load(f))\n",
        "    return geocache, processed_ids\n",
        "\n",
        "def save_cache(geocache, processed_ids):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à –≥–µ–æ–∫–æ–¥–æ–≤ –∏ –±—É—Ñ–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö ID —Å–æ–æ–±—â–µ–Ω–∏–π\"\"\"\n",
        "    with open(GEO_CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(geocache, f, ensure_ascii=False, indent=2)\n",
        "    with open(PROCESSED_IDS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(list(processed_ids), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_noise_buffer():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è-–≤—ã–±—Ä–æ—Å—ã (–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)\"\"\"\n",
        "    if os.path.exists(NOISE_BUFFER_FILE):\n",
        "        with open(NOISE_BUFFER_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            buffer = json.load(f)\n",
        "        now = datetime.now()\n",
        "        return [\n",
        "            msg for msg in buffer\n",
        "            if (now - datetime.fromisoformat(msg[\"buffered_at\"])) < timedelta(hours=NOISE_TTL_HOURS)\n",
        "        ]\n",
        "    return []\n",
        "\n",
        "def save_noise_buffer(buffer):\n",
        "    \"\"\"–°–æ—Ö–∞—Ä–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è-–≤—ã–±—Ä–æ—Å—ã (–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)\"\"\"\n",
        "    buffer_sorted = sorted(buffer, key=lambda x: x.get(\"buffered_at\", \"\"))\n",
        "    trimmed = buffer_sorted[-MAX_NOISE_BUFFER_SIZE:]\n",
        "    with open(NOISE_BUFFER_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(trimmed, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# def load_existing_clusters():\n",
        "#     # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ main(), –Ω–æ –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",
        "#     if not os.path.exists(CLUSTERS_PROCESSED_FILE):\n",
        "#         return []\n",
        "#     try:\n",
        "#         with open(CLUSTERS_PROCESSED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "#             data = json.load(f)\n",
        "#         clusters = data.get(\"clusters\", [])\n",
        "#         for cl in clusters:\n",
        "#             if isinstance(cl[\"core_embedding\"], list):\n",
        "#                 cl[\"core_embedding\"] = np.array(cl[\"core_embedding\"], dtype=np.float32)\n",
        "#         return clusters\n",
        "#     except Exception as e:\n",
        "#         print(f\"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã: {e}\")\n",
        "#         return []\n",
        "\n",
        "def save_clusters(clusters):\n",
        "    # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ main(), –Ω–æ –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",
        "    serializable = []\n",
        "    for cl in clusters:\n",
        "        cl_copy = cl.copy()\n",
        "        if isinstance(cl_copy[\"core_embedding\"], np.ndarray):\n",
        "            cl_copy[\"core_embedding\"] = cl_copy[\"core_embedding\"].tolist()\n",
        "        serializable.append(cl_copy)\n",
        "    with open(CLUSTERS_PROCESSED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"clusters\": serializable}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def update_cluster_with_message(cluster, new_msg, model, max_examples=5):\n",
        "    all_texts = [ex[\"text\"] for ex in cluster[\"examples\"]] + [new_msg[\"text\"]]\n",
        "    new_emb = np.mean(model.encode(all_texts, convert_to_numpy=True), axis=0)\n",
        "    updated_examples = (cluster[\"examples\"] + [new_msg])[-max_examples:]\n",
        "    cluster.update({\n",
        "        \"core_embedding\": new_emb,\n",
        "        \"message_count\": cluster[\"message_count\"] + 1,\n",
        "        \"last_seen\": max(cluster[\"last_seen\"], new_msg[\"date\"]),\n",
        "        \"examples\": updated_examples\n",
        "    })\n",
        "\n",
        "\n",
        "def aggregate_cluster_metadata(messages):\n",
        "    \"\"\"\n",
        "    –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ —Å–ø–∏—Å–∫—É —Å–æ–æ–±—â–µ–Ω–∏–π.\n",
        "    –û–∂–∏–¥–∞–µ—Ç, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç:\n",
        "      - \"category\" (str)\n",
        "      - \"sentiment\" (str: \"–Ω–µ–≥–∞—Ç–∏–≤\", \"–Ω–µ–π—Ç—Ä\", \"–ø–æ–∑–∏—Ç–∏–≤\")\n",
        "    \"\"\"\n",
        "    if not messages:\n",
        "        return {\n",
        "            \"category\": \"–¥—Ä—É–≥–æ–µ\",\n",
        "            \"sentiment_ratio\": {\"–Ω–µ–≥–∞—Ç–∏–≤\": 0.0, \"–Ω–µ–π—Ç—Ä\": 0.0, \"–ø–æ–∑–∏—Ç–∏–≤\": 0.0},\n",
        "            \"is_critical\": False\n",
        "        }\n",
        "\n",
        "    # –ö–∞—Ç–µ–≥–æ—Ä–∏—è: —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è\n",
        "    categories = [m.get(\"category\", \"–¥—Ä—É–≥–æ–µ\") for m in messages]\n",
        "    dominant_category = Counter(categories).most_common(1)[0][0]\n",
        "\n",
        "    # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: –¥–æ–ª—è –∫–∞–∂–¥–æ–π\n",
        "    sentiments = [m.get(\"sentiment\", \"–Ω–µ–π—Ç—Ä\") for m in messages]\n",
        "    total = len(sentiments)\n",
        "    sentiment_counts = Counter(sentiments)\n",
        "    sentiment_ratio = {\n",
        "        \"–Ω–µ–≥–∞—Ç–∏–≤\": round(sentiment_counts.get(\"–Ω–µ–≥–∞—Ç–∏–≤\", 0) / total, 2),\n",
        "        \"–Ω–µ–π—Ç—Ä\": round(sentiment_counts.get(\"–Ω–µ–π—Ç—Ä\", 0) / total, 2),\n",
        "        \"–ø–æ–∑–∏—Ç–∏–≤\": round(sentiment_counts.get(\"–ø–æ–∑–∏—Ç–∏–≤\", 0) / total, 2),\n",
        "    }\n",
        "\n",
        "    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω—Ü–∏–¥–µ–Ω—Ç: –º–Ω–æ–≥–æ –Ω–µ–≥–∞—Ç–∏–≤–∞ + –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π\n",
        "    # is_critical = (\n",
        "    #     sentiment_ratio[\"–Ω–µ–≥–∞—Ç–∏–≤\"] >= 0.6 and\n",
        "    #     total >= 2\n",
        "    # )\n",
        "    is_critical = any(msg[\"is_critical\"] for msg in messages)\n",
        "\n",
        "    is_negative_count = sentiment_counts.get(\"–Ω–µ–≥–∞—Ç–∏–≤\", 0)\n",
        "    is_critical_count = len([msg[\"is_critical\"] == True for msg in messages])\n",
        "    priority = is_critical_count * (1 + is_negative_count)\n",
        "\n",
        "    return {\n",
        "        \"category\": dominant_category,\n",
        "        \"sentiment_ratio\": sentiment_ratio,\n",
        "        \"is_critical\": is_critical,\n",
        "        \"priority\": priority\n",
        "    }\n",
        "\n",
        "def try_assign_to_existing_clusters(new_messages, existing_clusters, model, geo_threshold=0.5, sim_threshold=0.45):\n",
        "    unassigned = []\n",
        "    for msg in new_messages:\n",
        "        assigned = False\n",
        "        msg_emb = model.encode([msg[\"text\"]], convert_to_numpy=True)[0]\n",
        "        for cl in existing_clusters:\n",
        "            if not geocode_similarity(msg[\"geocode\"], cl[\"geocode\"], threshold=geo_threshold):\n",
        "                continue\n",
        "            sim = cosine_similarity([msg_emb], [cl[\"core_embedding\"]])[0][0]\n",
        "            if sim >= sim_threshold:\n",
        "                update_cluster_with_message(cl, msg, model)\n",
        "                assigned = True\n",
        "                break\n",
        "        if not assigned:\n",
        "            unassigned.append(msg)\n",
        "    return unassigned\n",
        "\n",
        "def mark_messages_as_processed(message_ids):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ, —á—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\"\"\"\n",
        "    print(f\"‚úÖ –ü–æ–º–µ—á–µ–Ω–æ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(message_ids)} —Å–æ–æ–±—â–µ–Ω–∏–π\")\n",
        "    with open(PROCESSED_IDS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(list(message_ids), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ======================\n",
        "# –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø\n",
        "# ======================\n",
        "\n",
        "def geocode_similarity(g1, g2, threshold=0.5):\n",
        "    if not g1 or not g2:\n",
        "        return False\n",
        "    s1, s2 = set(g1), set(g2)\n",
        "    return len(set(s1 & s2)) / len(set(s1 | s2)) >= threshold\n",
        "\n",
        "def cluster_with_geo_grouping(messages, model):\n",
        "    geo_groups = []\n",
        "    for msg in messages:\n",
        "        placed = False\n",
        "        for group in geo_groups:\n",
        "            if geocode_similarity(msg[\"geocode\"], group[0][\"geocode\"]):\n",
        "                group.append(msg)\n",
        "                placed = True\n",
        "                break\n",
        "        if not placed:\n",
        "            geo_groups.append([msg])\n",
        "\n",
        "    clusters, noise = [], []\n",
        "    for group in geo_groups:\n",
        "        if len(group) == 1:\n",
        "            noise.extend(group)\n",
        "            continue\n",
        "\n",
        "        texts = [m[\"text\"] for m in group]\n",
        "        emb = model.encode(texts, convert_to_numpy=True)\n",
        "        labels = DBSCAN(eps=EPS_DBSCAN, min_samples=MIN_SAMPLES, metric='cosine').fit(emb).labels_\n",
        "\n",
        "        clustered_ids = set()\n",
        "        for label in set(labels):\n",
        "            if label == -1:\n",
        "                continue\n",
        "            msgs = [group[i] for i, l in enumerate(labels) if l == label]\n",
        "            if len(msgs) >= 2:\n",
        "                core_emb = np.mean(model.encode([m[\"text\"] for m in msgs], convert_to_numpy=True), axis=0)\n",
        "                cluster_meta = aggregate_cluster_metadata(msgs)\n",
        "                clusters.append({\n",
        "                    \"cluster_id\": f\"cl_{len(clusters)+1:03d}\",\n",
        "                    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:\n",
        "                    \"category\": cluster_meta[\"category\"],\n",
        "                    \"sentiment_ratio\": cluster_meta[\"sentiment_ratio\"],\n",
        "                    \"is_critical\": cluster_meta[\"is_critical\"],\n",
        "                    \"priority\": cluster_meta[\"priority\"],\n",
        "                    # –ì–µ–æ–∫–æ–¥—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "                    \"geocode\": msgs[0][\"geocode\"],\n",
        "                    \"coords\": msgs[0][\"coords\"],\n",
        "                    \"message_count\": len(msgs),\n",
        "                    \"first_seen\": min(m[\"date\"] for m in msgs),\n",
        "                    \"last_seen\": max(m[\"date\"] for m in msgs),\n",
        "                    \"core_embedding\": core_emb.tolist(),\n",
        "                    \"examples\": msgs[-5:]\n",
        "                })\n",
        "                clustered_ids.update(m[\"id\"] for m in msgs)\n",
        "\n",
        "        noise.extend([m for m in group if m[\"id\"] not in clustered_ids])\n",
        "\n",
        "    return clusters, noise\n",
        "\n",
        "# ======================\n",
        "# MAIN (–†–ï–ñ–ò–ú POLLING –î–õ–Ø –î–ï–ú–û –ù–ê –•–ê–ö–ê–¢–û–ù–ï)\n",
        "# ======================\n",
        "\n",
        "def run_clustering_cycle():\n",
        "    \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –û–î–ò–ù —Ü–∏–∫–ª –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ ‚Üí –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç ‚Üí –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç.\"\"\"\n",
        "    print(\"üß† –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (Colab ‚Üí VPS)...\")\n",
        "\n",
        "    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç VPS\n",
        "    messages = fetch_messages_from_vps()\n",
        "    if not messages:\n",
        "        print(\"üí§ –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.\")\n",
        "        return False\n",
        "\n",
        "    # 2. –ó–ê–ì–†–£–ñ–ê–ï–ú –õ–û–ö–ê–õ–¨–ù–´–ô –ö–≠–® –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• ID (–¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –¥—É–±–ª–µ–π)\n",
        "    geocache, processed_ids_local = load_cache()\n",
        "\n",
        "    # 3. –§–ò–õ–¨–¢–†–£–ï–ú: –æ—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û –ù–û–í–´–ï —Å–æ–æ–±—â–µ–Ω–∏—è (–∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏—Å—å –≤ Colab —Ä–∞–Ω–µ–µ)\n",
        "    new_messages = [msg for msg in messages if msg[\"id\"] not in processed_ids_local]\n",
        "    if not new_messages:\n",
        "        print(\"‚è≠Ô∏è  –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã (–ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à).\")\n",
        "        return False\n",
        "    print(f\"üÜï –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(new_messages)} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–∏–∑ {len(messages)} –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö).\")\n",
        "\n",
        "    # 4. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –æ—Ç VPS (–∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ)\n",
        "    existing_clusters_raw = fetch_clusters_from_vps()\n",
        "    existing_clusters = []\n",
        "    for cl in existing_clusters_raw:\n",
        "        if isinstance(cl.get(\"core_embedding\"), list):\n",
        "            cl[\"core_embedding\"] = np.array(cl[\"core_embedding\"], dtype=np.float32)\n",
        "        existing_clusters.append(cl)\n",
        "\n",
        "    print(\"–ò–∑–≤–ª–µ–∫–∞–µ–º –≥–µ–æ–∫–æ–¥—ã –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã\")\n",
        "    # 5. –ì–µ–æ–∫–æ–¥–∏—Ä—É–µ–º, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    for msg in new_messages:\n",
        "        if \"geocode\" not in msg or not msg[\"geocode\"]:\n",
        "            geo_data = extract_geocodes_cached(msg[\"id\"], msg[\"text\"], geocache)\n",
        "            msg[\"geocode\"] = geo_data[\"geocode\"]\n",
        "            msg[\"coords\"] = geo_data[\"coords\"]\n",
        "\n",
        "    # 6. –û–ë–ù–û–í–õ–Ø–ï–ú –õ–û–ö–ê–õ–¨–ù–´–ô –ö–≠–® –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• ID\n",
        "    for msg in new_messages:\n",
        "        processed_ids_local.add(msg[\"id\"])\n",
        "    save_cache(geocache, processed_ids_local)\n",
        "\n",
        "    # 7. –ó–∞–≥—Ä—É–∂–∞–µ–º –±—É—Ñ–µ—Ä –≤—ã–±—Ä–æ—Å–æ–≤ (–ª–æ–∫–∞–ª—å–Ω—ã–π)\n",
        "    noise_buffer = load_noise_buffer()\n",
        "\n",
        "    # 8. –ú–æ–¥–µ–ª—å\n",
        "    model = SentenceTransformer(MODEL_NAME, device='cpu')\n",
        "\n",
        "    print(\"–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º\")\n",
        "    # 9. –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º\n",
        "    remaining_new = try_assign_to_existing_clusters(\n",
        "        new_messages,\n",
        "        existing_clusters,\n",
        "        model,\n",
        "        geo_threshold=0.3,\n",
        "        sim_threshold=TEXT_SIMILARITY_THRESHOLD\n",
        "    )\n",
        "\n",
        "    print(\"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –Ω–æ–≤—ã–µ –≤—ã–±—Ä–æ—Å—ã —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –≤—ã–±—Ä–æ—Å–∞–º–∏\")\n",
        "    # 10. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –æ—Å—Ç–∞—Ç–æ–∫\n",
        "    new_clusters, remaining_noise = cluster_with_geo_grouping(\n",
        "        remaining_new + noise_buffer, model\n",
        "    )\n",
        "\n",
        "    print(\"–û–±–Ω–æ–≤–ª—è–µ–º –±—É—Ñ–µ—Ä –≤—ã–±—Ä–æ—Å–æ–≤\")\n",
        "    # 11. –û–±–Ω–æ–≤–ª—è–µ–º –±—É—Ñ–µ—Ä\n",
        "    now = datetime.now().isoformat()\n",
        "    for msg in remaining_noise:\n",
        "        msg[\"buffered_at\"] = now\n",
        "    save_noise_buffer(remaining_noise)\n",
        "\n",
        "    # 12. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "    all_clusters = existing_clusters + new_clusters\n",
        "    clusters_for_vps = []\n",
        "    for cl in all_clusters:\n",
        "        cl_copy = cl.copy()\n",
        "        if isinstance(cl_copy.get(\"core_embedding\"), np.ndarray):\n",
        "            cl_copy[\"core_embedding\"] = cl_copy[\"core_embedding\"].tolist()\n",
        "        clusters_for_vps.append(cl_copy)\n",
        "\n",
        "    # 13. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ VPS\n",
        "    send_clusters_to_vps({\"clusters\": clusters_for_vps})\n",
        "\n",
        "    # 14. –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ù–ê VPS (–æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã)\n",
        "    mark_messages_as_processed({msg[\"id\"] for msg in new_messages})\n",
        "\n",
        "    print(f\"\\n‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω!\")\n",
        "    print(f\"  ‚Ä¢ –ù–æ–≤—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(new_clusters)}\")\n",
        "    print(f\"  ‚Ä¢ –û–±–Ω–æ–≤–ª–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(existing_clusters)}\")\n",
        "    print(f\"  ‚Ä¢ –í –±—É—Ñ–µ—Ä–µ: {len(remaining_noise)}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π polling-—Ü–∏–∫–ª –¥–ª—è –¥–µ–º–æ –Ω–∞ —Ö–∞–∫–∞—Ç–æ–Ω–µ.\"\"\"\n",
        "    print(\"üîÅ –ó–∞–ø—É—Å–∫ polling-—Ä–µ–∂–∏–º–∞ –¥–ª—è –¥–µ–º–æ (–Ω–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏)...\")\n",
        "    POLL_INTERVAL_SECONDS = 10  # ‚Üê –ù–∞—Å—Ç—Ä–æ–π –ø–æ —Å–≤–æ–µ–º—É —É—Å–º–æ—Ç—Ä–µ–Ω–∏—é\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            print(f\"\\n--- –ù–æ–≤—ã–π —Ü–∏–∫–ª (–æ–∂–∏–¥–∞–Ω–∏–µ {POLL_INTERVAL_SECONDS} —Å–µ–∫) ---\")\n",
        "            if not run_clustering_cycle():\n",
        "                print(f\"üí§ –°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ {POLL_INTERVAL_SECONDS} —Å–µ–∫—É–Ω–¥...\")\n",
        "                time.sleep(POLL_INTERVAL_SECONDS)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Polling –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ: {e}\")\n",
        "        # –í –¥–µ–º–æ –º–æ–∂–Ω–æ –ø–æ–∑–≤–æ–ª–∏—Ç—å –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è, –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ü–∏–∫–ª\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZpqijABB-vGT",
        "outputId": "6eec6fcc-8e58-4fde-a70a-a2a071dfe05f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ –ó–∞–ø—É—Å–∫ polling-—Ä–µ–∂–∏–º–∞ –¥–ª—è –¥–µ–º–æ (–Ω–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏)...\n",
            "\n",
            "--- –ù–æ–≤—ã–π —Ü–∏–∫–ª (–æ–∂–∏–¥–∞–Ω–∏–µ 10 —Å–µ–∫) ---\n",
            "üß† –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (Colab ‚Üí VPS)...\n",
            "üì• –ó–∞–ø—Ä–æ—Å –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å VPS...\n",
            "üÜï –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: 72 –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–∏–∑ 72 –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö).\n",
            "üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—É—â–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å VPS...\n",
            "–ò–∑–≤–ª–µ–∫–∞–µ–º –≥–µ–æ–∫–æ–¥—ã –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã\n",
            "–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º\n",
            "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –Ω–æ–≤—ã–µ –≤—ã–±—Ä–æ—Å—ã —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –≤—ã–±—Ä–æ—Å–∞–º–∏\n",
            "–û–±–Ω–æ–≤–ª—è–µ–º –±—É—Ñ–µ—Ä –≤—ã–±—Ä–æ—Å–æ–≤\n",
            "üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ VPS...\n",
            "‚úÖ –ü–æ–º–µ—á–µ–Ω–æ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 72 —Å–æ–æ–±—â–µ–Ω–∏–π\n",
            "\n",
            "‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω!\n",
            "  ‚Ä¢ –ù–æ–≤—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: 10\n",
            "  ‚Ä¢ –û–±–Ω–æ–≤–ª–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: 0\n",
            "  ‚Ä¢ –í –±—É—Ñ–µ—Ä–µ: 51\n",
            "\n",
            "--- –ù–æ–≤—ã–π —Ü–∏–∫–ª (–æ–∂–∏–¥–∞–Ω–∏–µ 10 —Å–µ–∫) ---\n",
            "üß† –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (Colab ‚Üí VPS)...\n",
            "üì• –ó–∞–ø—Ä–æ—Å –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å VPS...\n",
            "‚è≠Ô∏è  –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã (–ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à).\n",
            "üí§ –°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥...\n",
            "\n",
            "üõë Polling –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ml3aiOBixdcM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}